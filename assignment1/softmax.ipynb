{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clear previously loaded data.\n",
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.441292\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in*\n",
    "- Because there's 10 classes, by initialize with random weights, each class should has same possibility 1/10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -1.927047 analytic: -1.927047, relative error: 5.740587e-08\n",
      "numerical: -1.795655 analytic: -1.795655, relative error: 2.062075e-08\n",
      "numerical: -1.250615 analytic: -1.250614, relative error: 4.010706e-08\n",
      "numerical: 5.991641 analytic: 5.991640, relative error: 6.732212e-09\n",
      "numerical: -2.133731 analytic: -2.133731, relative error: 2.961581e-09\n",
      "numerical: -1.836689 analytic: -1.836689, relative error: 1.586974e-08\n",
      "numerical: 0.738910 analytic: 0.738910, relative error: 5.923412e-08\n",
      "numerical: 0.120966 analytic: 0.120966, relative error: 2.269411e-07\n",
      "numerical: -0.815862 analytic: -0.815862, relative error: 7.332057e-09\n",
      "numerical: 5.832936 analytic: 5.832936, relative error: 8.475831e-09\n",
      "numerical: -0.254732 analytic: -0.254732, relative error: 6.466996e-08\n",
      "numerical: -0.308471 analytic: -0.308470, relative error: 2.112811e-07\n",
      "numerical: 0.010004 analytic: 0.010003, relative error: 6.511927e-06\n",
      "numerical: -1.932408 analytic: -1.932408, relative error: 1.394059e-08\n",
      "numerical: -1.357689 analytic: -1.357690, relative error: 5.311052e-08\n",
      "numerical: 0.763532 analytic: 0.763532, relative error: 5.644395e-08\n",
      "numerical: 1.294893 analytic: 1.294893, relative error: 3.283125e-08\n",
      "numerical: -0.433203 analytic: -0.433203, relative error: 7.083255e-08\n",
      "numerical: 5.024381 analytic: 5.024381, relative error: 1.237280e-08\n",
      "numerical: -0.689831 analytic: -0.689831, relative error: 2.249179e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.441292e+00 computed in 0.099584s\n",
      "vectorized loss: 2.441292e+00 computed in 0.022390s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.0000001000\n",
      "regularization_strengths: 5000.0000000000\n",
      "iteration 0 / 1000: loss 82.449130\n",
      "iteration 100 / 1000: loss 73.449809\n",
      "iteration 200 / 1000: loss 65.974751\n",
      "iteration 300 / 1000: loss 59.899125\n",
      "iteration 400 / 1000: loss 53.692640\n",
      "iteration 500 / 1000: loss 48.738909\n",
      "iteration 600 / 1000: loss 44.295614\n",
      "iteration 700 / 1000: loss 40.053068\n",
      "iteration 800 / 1000: loss 36.331790\n",
      "iteration 900 / 1000: loss 32.918686\n",
      "training accuracy: 0.2604693878\n",
      "validation accuracy: 0.2490000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000001000\n",
      "regularization_strengths: 10000.0000000000\n",
      "iteration 0 / 1000: loss 157.907056\n",
      "iteration 100 / 1000: loss 128.967961\n",
      "iteration 200 / 1000: loss 105.239197\n",
      "iteration 300 / 1000: loss 86.240169\n",
      "iteration 400 / 1000: loss 70.527177\n",
      "iteration 500 / 1000: loss 58.171681\n",
      "iteration 600 / 1000: loss 47.751059\n",
      "iteration 700 / 1000: loss 39.444172\n",
      "iteration 800 / 1000: loss 32.655577\n",
      "iteration 900 / 1000: loss 26.977843\n",
      "training accuracy: 0.2866530612\n",
      "validation accuracy: 0.2870000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000001000\n",
      "regularization_strengths: 11000.0000000000\n",
      "iteration 0 / 1000: loss 175.684554\n",
      "iteration 100 / 1000: loss 140.589200\n",
      "iteration 200 / 1000: loss 112.703178\n",
      "iteration 300 / 1000: loss 90.608723\n",
      "iteration 400 / 1000: loss 72.720662\n",
      "iteration 500 / 1000: loss 58.830248\n",
      "iteration 600 / 1000: loss 47.432477\n",
      "iteration 700 / 1000: loss 38.268710\n",
      "iteration 800 / 1000: loss 31.024766\n",
      "iteration 900 / 1000: loss 25.339055\n",
      "training accuracy: 0.2971224490\n",
      "validation accuracy: 0.2990000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000001000\n",
      "regularization_strengths: 12000.0000000000\n",
      "iteration 0 / 1000: loss 192.620422\n",
      "iteration 100 / 1000: loss 150.123513\n",
      "iteration 200 / 1000: loss 118.041297\n",
      "iteration 300 / 1000: loss 92.966651\n",
      "iteration 400 / 1000: loss 73.503398\n",
      "iteration 500 / 1000: loss 57.970614\n",
      "iteration 600 / 1000: loss 45.862230\n",
      "iteration 700 / 1000: loss 36.464428\n",
      "iteration 800 / 1000: loss 29.130730\n",
      "iteration 900 / 1000: loss 23.223163\n",
      "training accuracy: 0.2974489796\n",
      "validation accuracy: 0.2910000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000001000\n",
      "regularization_strengths: 13000.0000000000\n",
      "iteration 0 / 1000: loss 208.060662\n",
      "iteration 100 / 1000: loss 158.823251\n",
      "iteration 200 / 1000: loss 122.857573\n",
      "iteration 300 / 1000: loss 94.717951\n",
      "iteration 400 / 1000: loss 73.280317\n",
      "iteration 500 / 1000: loss 56.792992\n",
      "iteration 600 / 1000: loss 44.179993\n",
      "iteration 700 / 1000: loss 34.351626\n",
      "iteration 800 / 1000: loss 27.048123\n",
      "iteration 900 / 1000: loss 21.030153\n",
      "training accuracy: 0.3066326531\n",
      "validation accuracy: 0.3150000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000001000\n",
      "regularization_strengths: 14000.0000000000\n",
      "iteration 0 / 1000: loss 220.247632\n",
      "iteration 100 / 1000: loss 165.539538\n",
      "iteration 200 / 1000: loss 125.114939\n",
      "iteration 300 / 1000: loss 94.923284\n",
      "iteration 400 / 1000: loss 71.994494\n",
      "iteration 500 / 1000: loss 54.657873\n",
      "iteration 600 / 1000: loss 41.751832\n",
      "iteration 700 / 1000: loss 31.991759\n",
      "iteration 800 / 1000: loss 24.589203\n",
      "iteration 900 / 1000: loss 19.047717\n",
      "training accuracy: 0.3071020408\n",
      "validation accuracy: 0.3030000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000001000\n",
      "regularization_strengths: 15000.0000000000\n",
      "iteration 0 / 1000: loss 235.814392\n",
      "iteration 100 / 1000: loss 172.686406\n",
      "iteration 200 / 1000: loss 127.719668\n",
      "iteration 300 / 1000: loss 94.910801\n",
      "iteration 400 / 1000: loss 70.549413\n",
      "iteration 500 / 1000: loss 52.664769\n",
      "iteration 600 / 1000: loss 39.359431\n",
      "iteration 700 / 1000: loss 29.661487\n",
      "iteration 800 / 1000: loss 22.450899\n",
      "iteration 900 / 1000: loss 17.121694\n",
      "training accuracy: 0.3142448980\n",
      "validation accuracy: 0.3260000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000001000\n",
      "regularization_strengths: 16000.0000000000\n",
      "iteration 0 / 1000: loss 249.557772\n",
      "iteration 100 / 1000: loss 179.882189\n",
      "iteration 200 / 1000: loss 130.560227\n",
      "iteration 300 / 1000: loss 94.937805\n",
      "iteration 400 / 1000: loss 69.402609\n",
      "iteration 500 / 1000: loss 50.875047\n",
      "iteration 600 / 1000: loss 37.335995\n",
      "iteration 700 / 1000: loss 27.614137\n",
      "iteration 800 / 1000: loss 20.622219\n",
      "iteration 900 / 1000: loss 15.521360\n",
      "training accuracy: 0.3213469388\n",
      "validation accuracy: 0.3360000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000001000\n",
      "regularization_strengths: 17000.0000000000\n",
      "iteration 0 / 1000: loss 263.140143\n",
      "iteration 100 / 1000: loss 186.572179\n",
      "iteration 200 / 1000: loss 132.757853\n",
      "iteration 300 / 1000: loss 94.865566\n",
      "iteration 400 / 1000: loss 67.923726\n",
      "iteration 500 / 1000: loss 48.757626\n",
      "iteration 600 / 1000: loss 35.192704\n",
      "iteration 700 / 1000: loss 25.599427\n",
      "iteration 800 / 1000: loss 18.794815\n",
      "iteration 900 / 1000: loss 13.883642\n",
      "training accuracy: 0.3200612245\n",
      "validation accuracy: 0.3260000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000001000\n",
      "regularization_strengths: 18000.0000000000\n",
      "iteration 0 / 1000: loss 287.448554\n",
      "iteration 100 / 1000: loss 198.499737\n",
      "iteration 200 / 1000: loss 138.749948\n",
      "iteration 300 / 1000: loss 97.183448\n",
      "iteration 400 / 1000: loss 68.248681\n",
      "iteration 500 / 1000: loss 48.068639\n",
      "iteration 600 / 1000: loss 34.065911\n",
      "iteration 700 / 1000: loss 24.327030\n",
      "iteration 800 / 1000: loss 17.482981\n",
      "iteration 900 / 1000: loss 12.805861\n",
      "training accuracy: 0.3228163265\n",
      "validation accuracy: 0.3050000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000001000\n",
      "regularization_strengths: 19000.0000000000\n",
      "iteration 0 / 1000: loss 299.196695\n",
      "iteration 100 / 1000: loss 203.324212\n",
      "iteration 200 / 1000: loss 139.190027\n",
      "iteration 300 / 1000: loss 95.386411\n",
      "iteration 400 / 1000: loss 65.831339\n",
      "iteration 500 / 1000: loss 45.597094\n",
      "iteration 600 / 1000: loss 31.602953\n",
      "iteration 700 / 1000: loss 22.232144\n",
      "iteration 800 / 1000: loss 15.856078\n",
      "iteration 900 / 1000: loss 11.367309\n",
      "training accuracy: 0.3302040816\n",
      "validation accuracy: 0.3490000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000001000\n",
      "regularization_strengths: 20000.0000000000\n",
      "iteration 0 / 1000: loss 312.129456\n",
      "iteration 100 / 1000: loss 208.546140\n",
      "iteration 200 / 1000: loss 140.142861\n",
      "iteration 300 / 1000: loss 94.210739\n",
      "iteration 400 / 1000: loss 63.669099\n",
      "iteration 500 / 1000: loss 43.165117\n",
      "iteration 600 / 1000: loss 29.543643\n",
      "iteration 700 / 1000: loss 20.409890\n",
      "iteration 800 / 1000: loss 14.365834\n",
      "iteration 900 / 1000: loss 10.217061\n",
      "training accuracy: 0.3247755102\n",
      "validation accuracy: 0.3410000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000001000\n",
      "regularization_strengths: 21000.0000000000\n",
      "iteration 0 / 1000: loss 326.912249\n",
      "iteration 100 / 1000: loss 213.631288\n",
      "iteration 200 / 1000: loss 140.806482\n",
      "iteration 300 / 1000: loss 92.798966\n",
      "iteration 400 / 1000: loss 61.364432\n",
      "iteration 500 / 1000: loss 41.016331\n",
      "iteration 600 / 1000: loss 27.557276\n",
      "iteration 700 / 1000: loss 18.699268\n",
      "iteration 800 / 1000: loss 13.059174\n",
      "iteration 900 / 1000: loss 9.289896\n",
      "training accuracy: 0.3315510204\n",
      "validation accuracy: 0.3300000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000001000\n",
      "regularization_strengths: 22000.0000000000\n",
      "iteration 0 / 1000: loss 345.656467\n",
      "iteration 100 / 1000: loss 221.911229\n",
      "iteration 200 / 1000: loss 143.007277\n",
      "iteration 300 / 1000: loss 92.615788\n",
      "iteration 400 / 1000: loss 60.033780\n",
      "iteration 500 / 1000: loss 39.311532\n",
      "iteration 600 / 1000: loss 25.963607\n",
      "iteration 700 / 1000: loss 17.457925\n",
      "iteration 800 / 1000: loss 11.801332\n",
      "iteration 900 / 1000: loss 8.404388\n",
      "training accuracy: 0.3346530612\n",
      "validation accuracy: 0.3550000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000001000\n",
      "regularization_strengths: 23000.0000000000\n",
      "iteration 0 / 1000: loss 362.296230\n",
      "iteration 100 / 1000: loss 227.954021\n",
      "iteration 200 / 1000: loss 144.042316\n",
      "iteration 300 / 1000: loss 91.263758\n",
      "iteration 400 / 1000: loss 58.420273\n",
      "iteration 500 / 1000: loss 37.491243\n",
      "iteration 600 / 1000: loss 24.282586\n",
      "iteration 700 / 1000: loss 16.090365\n",
      "iteration 800 / 1000: loss 10.844676\n",
      "iteration 900 / 1000: loss 7.615164\n",
      "training accuracy: 0.3347142857\n",
      "validation accuracy: 0.3410000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000001000\n",
      "regularization_strengths: 24000.0000000000\n",
      "iteration 0 / 1000: loss 374.215174\n",
      "iteration 100 / 1000: loss 231.070615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 200 / 1000: loss 143.108218\n",
      "iteration 300 / 1000: loss 89.099331\n",
      "iteration 400 / 1000: loss 55.802364\n",
      "iteration 500 / 1000: loss 35.212818\n",
      "iteration 600 / 1000: loss 22.475077\n",
      "iteration 700 / 1000: loss 14.575882\n",
      "iteration 800 / 1000: loss 9.794400\n",
      "iteration 900 / 1000: loss 6.872403\n",
      "training accuracy: 0.3345306122\n",
      "validation accuracy: 0.3500000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000001000\n",
      "regularization_strengths: 25000.0000000000\n",
      "iteration 0 / 1000: loss 392.514781\n",
      "iteration 100 / 1000: loss 237.265639\n",
      "iteration 200 / 1000: loss 143.967456\n",
      "iteration 300 / 1000: loss 87.875513\n",
      "iteration 400 / 1000: loss 53.877667\n",
      "iteration 500 / 1000: loss 33.396550\n",
      "iteration 600 / 1000: loss 20.973979\n",
      "iteration 700 / 1000: loss 13.583436\n",
      "iteration 800 / 1000: loss 8.943383\n",
      "iteration 900 / 1000: loss 6.164228\n",
      "training accuracy: 0.3341632653\n",
      "validation accuracy: 0.3550000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000001000\n",
      "regularization_strengths: 30000.0000000000\n",
      "iteration 0 / 1000: loss 471.128537\n",
      "iteration 100 / 1000: loss 257.720336\n",
      "iteration 200 / 1000: loss 141.862359\n",
      "iteration 300 / 1000: loss 78.446895\n",
      "iteration 400 / 1000: loss 43.778168\n",
      "iteration 500 / 1000: loss 24.830307\n",
      "iteration 600 / 1000: loss 14.544916\n",
      "iteration 700 / 1000: loss 9.017428\n",
      "iteration 800 / 1000: loss 5.807339\n",
      "iteration 900 / 1000: loss 4.030375\n",
      "training accuracy: 0.3397551020\n",
      "validation accuracy: 0.3580000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000001000\n",
      "regularization_strengths: 40000.0000000000\n",
      "iteration 0 / 1000: loss 621.872299\n",
      "iteration 100 / 1000: loss 278.896921\n",
      "iteration 200 / 1000: loss 125.923170\n",
      "iteration 300 / 1000: loss 57.415138\n",
      "iteration 400 / 1000: loss 26.865244\n",
      "iteration 500 / 1000: loss 13.155444\n",
      "iteration 600 / 1000: loss 6.996994\n",
      "iteration 700 / 1000: loss 4.281621\n",
      "iteration 800 / 1000: loss 3.083990\n",
      "iteration 900 / 1000: loss 2.507410\n",
      "training accuracy: 0.3341020408\n",
      "validation accuracy: 0.3500000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000004000\n",
      "regularization_strengths: 5000.0000000000\n",
      "iteration 0 / 1000: loss 81.494337\n",
      "iteration 100 / 1000: loss 53.482280\n",
      "iteration 200 / 1000: loss 36.316362\n",
      "iteration 300 / 1000: loss 24.668155\n",
      "iteration 400 / 1000: loss 17.012364\n",
      "iteration 500 / 1000: loss 12.105824\n",
      "iteration 600 / 1000: loss 8.735056\n",
      "iteration 700 / 1000: loss 6.491248\n",
      "iteration 800 / 1000: loss 4.867248\n",
      "iteration 900 / 1000: loss 3.923572\n",
      "training accuracy: 0.3775918367\n",
      "validation accuracy: 0.3660000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000004000\n",
      "regularization_strengths: 10000.0000000000\n",
      "iteration 0 / 1000: loss 159.131697\n",
      "iteration 100 / 1000: loss 70.545259\n",
      "iteration 200 / 1000: loss 32.489817\n",
      "iteration 300 / 1000: loss 15.599094\n",
      "iteration 400 / 1000: loss 8.047708\n",
      "iteration 500 / 1000: loss 4.596517\n",
      "iteration 600 / 1000: loss 3.190919\n",
      "iteration 700 / 1000: loss 2.437888\n",
      "iteration 800 / 1000: loss 2.102124\n",
      "iteration 900 / 1000: loss 2.038677\n",
      "training accuracy: 0.3705714286\n",
      "validation accuracy: 0.3740000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000004000\n",
      "regularization_strengths: 11000.0000000000\n",
      "iteration 0 / 1000: loss 171.784257\n",
      "iteration 100 / 1000: loss 71.116331\n",
      "iteration 200 / 1000: loss 30.334091\n",
      "iteration 300 / 1000: loss 13.691735\n",
      "iteration 400 / 1000: loss 6.677687\n",
      "iteration 500 / 1000: loss 3.981943\n",
      "iteration 600 / 1000: loss 2.726422\n",
      "iteration 700 / 1000: loss 2.238162\n",
      "iteration 800 / 1000: loss 2.084795\n",
      "iteration 900 / 1000: loss 1.935858\n",
      "training accuracy: 0.3672653061\n",
      "validation accuracy: 0.3910000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000004000\n",
      "regularization_strengths: 12000.0000000000\n",
      "iteration 0 / 1000: loss 194.143134\n",
      "iteration 100 / 1000: loss 73.062579\n",
      "iteration 200 / 1000: loss 28.962234\n",
      "iteration 300 / 1000: loss 12.318165\n",
      "iteration 400 / 1000: loss 5.867902\n",
      "iteration 500 / 1000: loss 3.462400\n",
      "iteration 600 / 1000: loss 2.574661\n",
      "iteration 700 / 1000: loss 2.160313\n",
      "iteration 800 / 1000: loss 1.995506\n",
      "iteration 900 / 1000: loss 1.927873\n",
      "training accuracy: 0.3727142857\n",
      "validation accuracy: 0.3810000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000004000\n",
      "regularization_strengths: 13000.0000000000\n",
      "iteration 0 / 1000: loss 203.332117\n",
      "iteration 100 / 1000: loss 71.711501\n",
      "iteration 200 / 1000: loss 26.292839\n",
      "iteration 300 / 1000: loss 10.476692\n",
      "iteration 400 / 1000: loss 4.907514\n",
      "iteration 500 / 1000: loss 3.088517\n",
      "iteration 600 / 1000: loss 2.386754\n",
      "iteration 700 / 1000: loss 2.074386\n",
      "iteration 800 / 1000: loss 2.021353\n",
      "iteration 900 / 1000: loss 2.008039\n",
      "training accuracy: 0.3657755102\n",
      "validation accuracy: 0.3770000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000004000\n",
      "regularization_strengths: 14000.0000000000\n",
      "iteration 0 / 1000: loss 216.839171\n",
      "iteration 100 / 1000: loss 70.876244\n",
      "iteration 200 / 1000: loss 24.248747\n",
      "iteration 300 / 1000: loss 9.200702\n",
      "iteration 400 / 1000: loss 4.345035\n",
      "iteration 500 / 1000: loss 2.700182\n",
      "iteration 600 / 1000: loss 2.215373\n",
      "iteration 700 / 1000: loss 2.062973\n",
      "iteration 800 / 1000: loss 1.987448\n",
      "iteration 900 / 1000: loss 2.007184\n",
      "training accuracy: 0.3658367347\n",
      "validation accuracy: 0.3770000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000004000\n",
      "regularization_strengths: 15000.0000000000\n",
      "iteration 0 / 1000: loss 237.321903\n",
      "iteration 100 / 1000: loss 71.210337\n",
      "iteration 200 / 1000: loss 22.573346\n",
      "iteration 300 / 1000: loss 8.238557\n",
      "iteration 400 / 1000: loss 3.801306\n",
      "iteration 500 / 1000: loss 2.519435\n",
      "iteration 600 / 1000: loss 2.102303\n",
      "iteration 700 / 1000: loss 2.003164\n",
      "iteration 800 / 1000: loss 2.042768\n",
      "iteration 900 / 1000: loss 1.956456\n",
      "training accuracy: 0.3597755102\n",
      "validation accuracy: 0.3720000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000004000\n",
      "regularization_strengths: 16000.0000000000\n",
      "iteration 0 / 1000: loss 253.957831\n",
      "iteration 100 / 1000: loss 69.652863\n",
      "iteration 200 / 1000: loss 20.558237\n",
      "iteration 300 / 1000: loss 7.091854\n",
      "iteration 400 / 1000: loss 3.433178\n",
      "iteration 500 / 1000: loss 2.429504\n",
      "iteration 600 / 1000: loss 2.088180\n",
      "iteration 700 / 1000: loss 2.070143\n",
      "iteration 800 / 1000: loss 1.991269\n",
      "iteration 900 / 1000: loss 1.962280\n",
      "training accuracy: 0.3585918367\n",
      "validation accuracy: 0.3860000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000004000\n",
      "regularization_strengths: 17000.0000000000\n",
      "iteration 0 / 1000: loss 265.166571\n",
      "iteration 100 / 1000: loss 68.201181\n",
      "iteration 200 / 1000: loss 18.706440\n",
      "iteration 300 / 1000: loss 6.215001\n",
      "iteration 400 / 1000: loss 3.063346\n",
      "iteration 500 / 1000: loss 2.273910\n",
      "iteration 600 / 1000: loss 2.016942\n",
      "iteration 700 / 1000: loss 2.060753\n",
      "iteration 800 / 1000: loss 2.092789\n",
      "iteration 900 / 1000: loss 2.000878\n",
      "training accuracy: 0.3585306122\n",
      "validation accuracy: 0.3720000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000004000\n",
      "regularization_strengths: 18000.0000000000\n",
      "iteration 0 / 1000: loss 282.692045\n",
      "iteration 100 / 1000: loss 67.170548\n",
      "iteration 200 / 1000: loss 17.204007\n",
      "iteration 300 / 1000: loss 5.648925\n",
      "iteration 400 / 1000: loss 2.832788\n",
      "iteration 500 / 1000: loss 2.236697\n",
      "iteration 600 / 1000: loss 1.998862\n",
      "iteration 700 / 1000: loss 1.980203\n",
      "iteration 800 / 1000: loss 2.059868\n",
      "iteration 900 / 1000: loss 2.039851\n",
      "training accuracy: 0.3586734694\n",
      "validation accuracy: 0.3720000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000004000\n",
      "regularization_strengths: 19000.0000000000\n",
      "iteration 0 / 1000: loss 300.682312\n",
      "iteration 100 / 1000: loss 66.029547\n",
      "iteration 200 / 1000: loss 15.768271\n",
      "iteration 300 / 1000: loss 4.984757\n",
      "iteration 400 / 1000: loss 2.544871\n",
      "iteration 500 / 1000: loss 2.094266\n",
      "iteration 600 / 1000: loss 2.048586\n",
      "iteration 700 / 1000: loss 1.993192\n",
      "iteration 800 / 1000: loss 1.966575\n",
      "iteration 900 / 1000: loss 2.051763\n",
      "training accuracy: 0.3581632653\n",
      "validation accuracy: 0.3650000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000004000\n",
      "regularization_strengths: 20000.0000000000\n",
      "iteration 0 / 1000: loss 313.508163\n",
      "iteration 100 / 1000: loss 63.425551\n",
      "iteration 200 / 1000: loss 14.242499\n",
      "iteration 300 / 1000: loss 4.411795\n",
      "iteration 400 / 1000: loss 2.499015\n",
      "iteration 500 / 1000: loss 2.145628\n",
      "iteration 600 / 1000: loss 2.011968\n",
      "iteration 700 / 1000: loss 2.000033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 800 / 1000: loss 2.012745\n",
      "iteration 900 / 1000: loss 1.991005\n",
      "training accuracy: 0.3614489796\n",
      "validation accuracy: 0.3710000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000004000\n",
      "regularization_strengths: 21000.0000000000\n",
      "iteration 0 / 1000: loss 326.512635\n",
      "iteration 100 / 1000: loss 61.107445\n",
      "iteration 200 / 1000: loss 12.864948\n",
      "iteration 300 / 1000: loss 4.068283\n",
      "iteration 400 / 1000: loss 2.376390\n",
      "iteration 500 / 1000: loss 2.118594\n",
      "iteration 600 / 1000: loss 2.019231\n",
      "iteration 700 / 1000: loss 2.039924\n",
      "iteration 800 / 1000: loss 2.075713\n",
      "iteration 900 / 1000: loss 2.062933\n",
      "training accuracy: 0.3582857143\n",
      "validation accuracy: 0.3720000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000004000\n",
      "regularization_strengths: 22000.0000000000\n",
      "iteration 0 / 1000: loss 344.280981\n",
      "iteration 100 / 1000: loss 59.547948\n",
      "iteration 200 / 1000: loss 11.805648\n",
      "iteration 300 / 1000: loss 3.679468\n",
      "iteration 400 / 1000: loss 2.324205\n",
      "iteration 500 / 1000: loss 2.076608\n",
      "iteration 600 / 1000: loss 1.969239\n",
      "iteration 700 / 1000: loss 2.062201\n",
      "iteration 800 / 1000: loss 2.042461\n",
      "iteration 900 / 1000: loss 2.061540\n",
      "training accuracy: 0.3400408163\n",
      "validation accuracy: 0.3510000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000004000\n",
      "regularization_strengths: 23000.0000000000\n",
      "iteration 0 / 1000: loss 358.010193\n",
      "iteration 100 / 1000: loss 57.265113\n",
      "iteration 200 / 1000: loss 10.665375\n",
      "iteration 300 / 1000: loss 3.290672\n",
      "iteration 400 / 1000: loss 2.247453\n",
      "iteration 500 / 1000: loss 1.974089\n",
      "iteration 600 / 1000: loss 1.998454\n",
      "iteration 700 / 1000: loss 2.002994\n",
      "iteration 800 / 1000: loss 2.019133\n",
      "iteration 900 / 1000: loss 2.052006\n",
      "training accuracy: 0.3457959184\n",
      "validation accuracy: 0.3630000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000004000\n",
      "regularization_strengths: 24000.0000000000\n",
      "iteration 0 / 1000: loss 373.110100\n",
      "iteration 100 / 1000: loss 55.352143\n",
      "iteration 200 / 1000: loss 9.742389\n",
      "iteration 300 / 1000: loss 3.179480\n",
      "iteration 400 / 1000: loss 2.226436\n",
      "iteration 500 / 1000: loss 2.103743\n",
      "iteration 600 / 1000: loss 2.082495\n",
      "iteration 700 / 1000: loss 2.069343\n",
      "iteration 800 / 1000: loss 2.090183\n",
      "iteration 900 / 1000: loss 1.990357\n",
      "training accuracy: 0.3544897959\n",
      "validation accuracy: 0.3690000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000004000\n",
      "regularization_strengths: 25000.0000000000\n",
      "iteration 0 / 1000: loss 390.002234\n",
      "iteration 100 / 1000: loss 53.318439\n",
      "iteration 200 / 1000: loss 8.773156\n",
      "iteration 300 / 1000: loss 2.930305\n",
      "iteration 400 / 1000: loss 2.134605\n",
      "iteration 500 / 1000: loss 2.021340\n",
      "iteration 600 / 1000: loss 2.053629\n",
      "iteration 700 / 1000: loss 2.103930\n",
      "iteration 800 / 1000: loss 2.069690\n",
      "iteration 900 / 1000: loss 2.009385\n",
      "training accuracy: 0.3528979592\n",
      "validation accuracy: 0.3730000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000004000\n",
      "regularization_strengths: 30000.0000000000\n",
      "iteration 0 / 1000: loss 462.564052\n",
      "iteration 100 / 1000: loss 42.671895\n",
      "iteration 200 / 1000: loss 5.714354\n",
      "iteration 300 / 1000: loss 2.355195\n",
      "iteration 400 / 1000: loss 2.105638\n",
      "iteration 500 / 1000: loss 2.092257\n",
      "iteration 600 / 1000: loss 2.043545\n",
      "iteration 700 / 1000: loss 2.022237\n",
      "iteration 800 / 1000: loss 2.119097\n",
      "iteration 900 / 1000: loss 2.123530\n",
      "training accuracy: 0.3436530612\n",
      "validation accuracy: 0.3530000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000004000\n",
      "regularization_strengths: 40000.0000000000\n",
      "iteration 0 / 1000: loss 621.559078\n",
      "iteration 100 / 1000: loss 26.367871\n",
      "iteration 200 / 1000: loss 3.090793\n",
      "iteration 300 / 1000: loss 2.142028\n",
      "iteration 400 / 1000: loss 2.079848\n",
      "iteration 500 / 1000: loss 2.075664\n",
      "iteration 600 / 1000: loss 2.086204\n",
      "iteration 700 / 1000: loss 2.085322\n",
      "iteration 800 / 1000: loss 2.101430\n",
      "iteration 900 / 1000: loss 2.062636\n",
      "training accuracy: 0.3337142857\n",
      "validation accuracy: 0.3370000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000008000\n",
      "regularization_strengths: 5000.0000000000\n",
      "iteration 0 / 1000: loss 83.202574\n",
      "iteration 100 / 1000: loss 36.622197\n",
      "iteration 200 / 1000: loss 17.280545\n",
      "iteration 300 / 1000: loss 8.700522\n",
      "iteration 400 / 1000: loss 4.936764\n",
      "iteration 500 / 1000: loss 3.209975\n",
      "iteration 600 / 1000: loss 2.400905\n",
      "iteration 700 / 1000: loss 2.067807\n",
      "iteration 800 / 1000: loss 2.103859\n",
      "iteration 900 / 1000: loss 1.832567\n",
      "training accuracy: 0.3816122449\n",
      "validation accuracy: 0.3850000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000008000\n",
      "regularization_strengths: 10000.0000000000\n",
      "iteration 0 / 1000: loss 160.829810\n",
      "iteration 100 / 1000: loss 32.840302\n",
      "iteration 200 / 1000: loss 8.050616\n",
      "iteration 300 / 1000: loss 3.215819\n",
      "iteration 400 / 1000: loss 2.110533\n",
      "iteration 500 / 1000: loss 2.012031\n",
      "iteration 600 / 1000: loss 1.970768\n",
      "iteration 700 / 1000: loss 1.941728\n",
      "iteration 800 / 1000: loss 1.919980\n",
      "iteration 900 / 1000: loss 1.904704\n",
      "training accuracy: 0.3704897959\n",
      "validation accuracy: 0.3720000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000008000\n",
      "regularization_strengths: 11000.0000000000\n",
      "iteration 0 / 1000: loss 175.401975\n",
      "iteration 100 / 1000: loss 30.863704\n",
      "iteration 200 / 1000: loss 6.782641\n",
      "iteration 300 / 1000: loss 2.897205\n",
      "iteration 400 / 1000: loss 2.093645\n",
      "iteration 500 / 1000: loss 1.978155\n",
      "iteration 600 / 1000: loss 2.039414\n",
      "iteration 700 / 1000: loss 2.095037\n",
      "iteration 800 / 1000: loss 1.915918\n",
      "iteration 900 / 1000: loss 1.949312\n",
      "training accuracy: 0.3592040816\n",
      "validation accuracy: 0.3770000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000008000\n",
      "regularization_strengths: 12000.0000000000\n",
      "iteration 0 / 1000: loss 192.812178\n",
      "iteration 100 / 1000: loss 28.848458\n",
      "iteration 200 / 1000: loss 5.906739\n",
      "iteration 300 / 1000: loss 2.499400\n",
      "iteration 400 / 1000: loss 2.077712\n",
      "iteration 500 / 1000: loss 1.979276\n",
      "iteration 600 / 1000: loss 2.014051\n",
      "iteration 700 / 1000: loss 1.877112\n",
      "iteration 800 / 1000: loss 2.044895\n",
      "iteration 900 / 1000: loss 1.985227\n",
      "training accuracy: 0.3667551020\n",
      "validation accuracy: 0.3750000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000008000\n",
      "regularization_strengths: 13000.0000000000\n",
      "iteration 0 / 1000: loss 207.179672\n",
      "iteration 100 / 1000: loss 26.460245\n",
      "iteration 200 / 1000: loss 4.976881\n",
      "iteration 300 / 1000: loss 2.279391\n",
      "iteration 400 / 1000: loss 2.053152\n",
      "iteration 500 / 1000: loss 2.063854\n",
      "iteration 600 / 1000: loss 1.970502\n",
      "iteration 700 / 1000: loss 1.902221\n",
      "iteration 800 / 1000: loss 2.001200\n",
      "iteration 900 / 1000: loss 1.994609\n",
      "training accuracy: 0.3579591837\n",
      "validation accuracy: 0.3750000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000008000\n",
      "regularization_strengths: 14000.0000000000\n",
      "iteration 0 / 1000: loss 219.921449\n",
      "iteration 100 / 1000: loss 24.302833\n",
      "iteration 200 / 1000: loss 4.300839\n",
      "iteration 300 / 1000: loss 2.142892\n",
      "iteration 400 / 1000: loss 1.983228\n",
      "iteration 500 / 1000: loss 2.033068\n",
      "iteration 600 / 1000: loss 2.003445\n",
      "iteration 700 / 1000: loss 1.975401\n",
      "iteration 800 / 1000: loss 1.965125\n",
      "iteration 900 / 1000: loss 1.954857\n",
      "training accuracy: 0.3592244898\n",
      "validation accuracy: 0.3720000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000008000\n",
      "regularization_strengths: 15000.0000000000\n",
      "iteration 0 / 1000: loss 235.829674\n",
      "iteration 100 / 1000: loss 22.350287\n",
      "iteration 200 / 1000: loss 3.756342\n",
      "iteration 300 / 1000: loss 2.170389\n",
      "iteration 400 / 1000: loss 1.985880\n",
      "iteration 500 / 1000: loss 1.916637\n",
      "iteration 600 / 1000: loss 2.039208\n",
      "iteration 700 / 1000: loss 1.927341\n",
      "iteration 800 / 1000: loss 2.029376\n",
      "iteration 900 / 1000: loss 1.981308\n",
      "training accuracy: 0.3666938776\n",
      "validation accuracy: 0.3810000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000008000\n",
      "regularization_strengths: 16000.0000000000\n",
      "iteration 0 / 1000: loss 253.985352\n",
      "iteration 100 / 1000: loss 20.695913\n",
      "iteration 200 / 1000: loss 3.398192\n",
      "iteration 300 / 1000: loss 2.086986\n",
      "iteration 400 / 1000: loss 2.034915\n",
      "iteration 500 / 1000: loss 1.981828\n",
      "iteration 600 / 1000: loss 1.965650\n",
      "iteration 700 / 1000: loss 2.051634\n",
      "iteration 800 / 1000: loss 1.963162\n",
      "iteration 900 / 1000: loss 2.036906\n",
      "training accuracy: 0.3497755102\n",
      "validation accuracy: 0.3710000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000008000\n",
      "regularization_strengths: 17000.0000000000\n",
      "iteration 0 / 1000: loss 266.727344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 1000: loss 18.676038\n",
      "iteration 200 / 1000: loss 3.044115\n",
      "iteration 300 / 1000: loss 2.008794\n",
      "iteration 400 / 1000: loss 1.963477\n",
      "iteration 500 / 1000: loss 2.017678\n",
      "iteration 600 / 1000: loss 1.937459\n",
      "iteration 700 / 1000: loss 1.994301\n",
      "iteration 800 / 1000: loss 2.030584\n",
      "iteration 900 / 1000: loss 1.965702\n",
      "training accuracy: 0.3564897959\n",
      "validation accuracy: 0.3690000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000008000\n",
      "regularization_strengths: 18000.0000000000\n",
      "iteration 0 / 1000: loss 278.866807\n",
      "iteration 100 / 1000: loss 16.922535\n",
      "iteration 200 / 1000: loss 2.794835\n",
      "iteration 300 / 1000: loss 2.074791\n",
      "iteration 400 / 1000: loss 2.036254\n",
      "iteration 500 / 1000: loss 2.059248\n",
      "iteration 600 / 1000: loss 1.971209\n",
      "iteration 700 / 1000: loss 2.036307\n",
      "iteration 800 / 1000: loss 1.989056\n",
      "iteration 900 / 1000: loss 2.020224\n",
      "training accuracy: 0.3547959184\n",
      "validation accuracy: 0.3770000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000008000\n",
      "regularization_strengths: 19000.0000000000\n",
      "iteration 0 / 1000: loss 296.752579\n",
      "iteration 100 / 1000: loss 15.398995\n",
      "iteration 200 / 1000: loss 2.645470\n",
      "iteration 300 / 1000: loss 2.053461\n",
      "iteration 400 / 1000: loss 1.978475\n",
      "iteration 500 / 1000: loss 1.985391\n",
      "iteration 600 / 1000: loss 1.978956\n",
      "iteration 700 / 1000: loss 1.980716\n",
      "iteration 800 / 1000: loss 2.063460\n",
      "iteration 900 / 1000: loss 2.031800\n",
      "training accuracy: 0.3516938776\n",
      "validation accuracy: 0.3720000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000008000\n",
      "regularization_strengths: 20000.0000000000\n",
      "iteration 0 / 1000: loss 312.668989\n",
      "iteration 100 / 1000: loss 14.059699\n",
      "iteration 200 / 1000: loss 2.503176\n",
      "iteration 300 / 1000: loss 2.067074\n",
      "iteration 400 / 1000: loss 1.955182\n",
      "iteration 500 / 1000: loss 2.046286\n",
      "iteration 600 / 1000: loss 2.009718\n",
      "iteration 700 / 1000: loss 1.987383\n",
      "iteration 800 / 1000: loss 1.989525\n",
      "iteration 900 / 1000: loss 2.045319\n",
      "training accuracy: 0.3485918367\n",
      "validation accuracy: 0.3590000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000008000\n",
      "regularization_strengths: 21000.0000000000\n",
      "iteration 0 / 1000: loss 333.889769\n",
      "iteration 100 / 1000: loss 12.906929\n",
      "iteration 200 / 1000: loss 2.348579\n",
      "iteration 300 / 1000: loss 2.015390\n",
      "iteration 400 / 1000: loss 2.080313\n",
      "iteration 500 / 1000: loss 2.009980\n",
      "iteration 600 / 1000: loss 1.983484\n",
      "iteration 700 / 1000: loss 2.138473\n",
      "iteration 800 / 1000: loss 2.053203\n",
      "iteration 900 / 1000: loss 2.039284\n",
      "training accuracy: 0.3525306122\n",
      "validation accuracy: 0.3680000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000008000\n",
      "regularization_strengths: 22000.0000000000\n",
      "iteration 0 / 1000: loss 345.438333\n",
      "iteration 100 / 1000: loss 11.641308\n",
      "iteration 200 / 1000: loss 2.361373\n",
      "iteration 300 / 1000: loss 2.102164\n",
      "iteration 400 / 1000: loss 1.987866\n",
      "iteration 500 / 1000: loss 2.014326\n",
      "iteration 600 / 1000: loss 1.990705\n",
      "iteration 700 / 1000: loss 1.946263\n",
      "iteration 800 / 1000: loss 1.953423\n",
      "iteration 900 / 1000: loss 2.029456\n",
      "training accuracy: 0.3440000000\n",
      "validation accuracy: 0.3480000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000008000\n",
      "regularization_strengths: 23000.0000000000\n",
      "iteration 0 / 1000: loss 358.817994\n",
      "iteration 100 / 1000: loss 10.465686\n",
      "iteration 200 / 1000: loss 2.220913\n",
      "iteration 300 / 1000: loss 2.059857\n",
      "iteration 400 / 1000: loss 2.019204\n",
      "iteration 500 / 1000: loss 1.992215\n",
      "iteration 600 / 1000: loss 2.071304\n",
      "iteration 700 / 1000: loss 1.986648\n",
      "iteration 800 / 1000: loss 1.982908\n",
      "iteration 900 / 1000: loss 2.080767\n",
      "training accuracy: 0.3541632653\n",
      "validation accuracy: 0.3630000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000008000\n",
      "regularization_strengths: 24000.0000000000\n",
      "iteration 0 / 1000: loss 376.119186\n",
      "iteration 100 / 1000: loss 9.602095\n",
      "iteration 200 / 1000: loss 2.279434\n",
      "iteration 300 / 1000: loss 2.062022\n",
      "iteration 400 / 1000: loss 1.984247\n",
      "iteration 500 / 1000: loss 2.036980\n",
      "iteration 600 / 1000: loss 2.033623\n",
      "iteration 700 / 1000: loss 2.092372\n",
      "iteration 800 / 1000: loss 2.038207\n",
      "iteration 900 / 1000: loss 1.987653\n",
      "training accuracy: 0.3392653061\n",
      "validation accuracy: 0.3480000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000008000\n",
      "regularization_strengths: 25000.0000000000\n",
      "iteration 0 / 1000: loss 387.784556\n",
      "iteration 100 / 1000: loss 8.651161\n",
      "iteration 200 / 1000: loss 2.218760\n",
      "iteration 300 / 1000: loss 2.022847\n",
      "iteration 400 / 1000: loss 2.046534\n",
      "iteration 500 / 1000: loss 2.060670\n",
      "iteration 600 / 1000: loss 2.037412\n",
      "iteration 700 / 1000: loss 2.008424\n",
      "iteration 800 / 1000: loss 2.047429\n",
      "iteration 900 / 1000: loss 1.997922\n",
      "training accuracy: 0.3319795918\n",
      "validation accuracy: 0.3520000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000008000\n",
      "regularization_strengths: 30000.0000000000\n",
      "iteration 0 / 1000: loss 466.344863\n",
      "iteration 100 / 1000: loss 5.653479\n",
      "iteration 200 / 1000: loss 2.081164\n",
      "iteration 300 / 1000: loss 2.115179\n",
      "iteration 400 / 1000: loss 2.034573\n",
      "iteration 500 / 1000: loss 2.060870\n",
      "iteration 600 / 1000: loss 2.027294\n",
      "iteration 700 / 1000: loss 2.093271\n",
      "iteration 800 / 1000: loss 2.035208\n",
      "iteration 900 / 1000: loss 2.070018\n",
      "training accuracy: 0.3422040816\n",
      "validation accuracy: 0.3640000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000008000\n",
      "regularization_strengths: 40000.0000000000\n",
      "iteration 0 / 1000: loss 619.737347\n",
      "iteration 100 / 1000: loss 2.952418\n",
      "iteration 200 / 1000: loss 2.011171\n",
      "iteration 300 / 1000: loss 2.104107\n",
      "iteration 400 / 1000: loss 2.083954\n",
      "iteration 500 / 1000: loss 2.023947\n",
      "iteration 600 / 1000: loss 2.022223\n",
      "iteration 700 / 1000: loss 2.184316\n",
      "iteration 800 / 1000: loss 2.085306\n",
      "iteration 900 / 1000: loss 2.087797\n",
      "training accuracy: 0.3267346939\n",
      "validation accuracy: 0.3460000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000010000\n",
      "regularization_strengths: 5000.0000000000\n",
      "iteration 0 / 1000: loss 83.042452\n",
      "iteration 100 / 1000: loss 29.809019\n",
      "iteration 200 / 1000: loss 11.947536\n",
      "iteration 300 / 1000: loss 5.547878\n",
      "iteration 400 / 1000: loss 3.168588\n",
      "iteration 500 / 1000: loss 2.356578\n",
      "iteration 600 / 1000: loss 2.032552\n",
      "iteration 700 / 1000: loss 1.891400\n",
      "iteration 800 / 1000: loss 1.953343\n",
      "iteration 900 / 1000: loss 1.872617\n",
      "training accuracy: 0.3859591837\n",
      "validation accuracy: 0.3870000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000010000\n",
      "regularization_strengths: 10000.0000000000\n",
      "iteration 0 / 1000: loss 161.441670\n",
      "iteration 100 / 1000: loss 22.462624\n",
      "iteration 200 / 1000: loss 4.619449\n",
      "iteration 300 / 1000: loss 2.321724\n",
      "iteration 400 / 1000: loss 1.937794\n",
      "iteration 500 / 1000: loss 2.016246\n",
      "iteration 600 / 1000: loss 1.860863\n",
      "iteration 700 / 1000: loss 1.918542\n",
      "iteration 800 / 1000: loss 1.893755\n",
      "iteration 900 / 1000: loss 1.971980\n",
      "training accuracy: 0.3630816327\n",
      "validation accuracy: 0.3760000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000010000\n",
      "regularization_strengths: 11000.0000000000\n",
      "iteration 0 / 1000: loss 175.603161\n",
      "iteration 100 / 1000: loss 20.439631\n",
      "iteration 200 / 1000: loss 3.974044\n",
      "iteration 300 / 1000: loss 2.104465\n",
      "iteration 400 / 1000: loss 1.999280\n",
      "iteration 500 / 1000: loss 1.913892\n",
      "iteration 600 / 1000: loss 1.911249\n",
      "iteration 700 / 1000: loss 2.010781\n",
      "iteration 800 / 1000: loss 2.009130\n",
      "iteration 900 / 1000: loss 1.932660\n",
      "training accuracy: 0.3684081633\n",
      "validation accuracy: 0.3780000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000010000\n",
      "regularization_strengths: 12000.0000000000\n",
      "iteration 0 / 1000: loss 193.554060\n",
      "iteration 100 / 1000: loss 18.536485\n",
      "iteration 200 / 1000: loss 3.426839\n",
      "iteration 300 / 1000: loss 2.176356\n",
      "iteration 400 / 1000: loss 2.001924\n",
      "iteration 500 / 1000: loss 1.945614\n",
      "iteration 600 / 1000: loss 1.961549\n",
      "iteration 700 / 1000: loss 1.905656\n",
      "iteration 800 / 1000: loss 2.000185\n",
      "iteration 900 / 1000: loss 2.009414\n",
      "training accuracy: 0.3645918367\n",
      "validation accuracy: 0.3810000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000010000\n",
      "regularization_strengths: 13000.0000000000\n",
      "iteration 0 / 1000: loss 206.914413\n",
      "iteration 100 / 1000: loss 16.518178\n",
      "iteration 200 / 1000: loss 3.144480\n",
      "iteration 300 / 1000: loss 2.068174\n",
      "iteration 400 / 1000: loss 2.000388\n",
      "iteration 500 / 1000: loss 2.054982\n",
      "iteration 600 / 1000: loss 1.943796\n",
      "iteration 700 / 1000: loss 1.979663\n",
      "iteration 800 / 1000: loss 1.963004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 900 / 1000: loss 1.887528\n",
      "training accuracy: 0.3630612245\n",
      "validation accuracy: 0.3660000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000010000\n",
      "regularization_strengths: 14000.0000000000\n",
      "iteration 0 / 1000: loss 219.131322\n",
      "iteration 100 / 1000: loss 14.633562\n",
      "iteration 200 / 1000: loss 2.695043\n",
      "iteration 300 / 1000: loss 2.008912\n",
      "iteration 400 / 1000: loss 1.952753\n",
      "iteration 500 / 1000: loss 1.923506\n",
      "iteration 600 / 1000: loss 1.988773\n",
      "iteration 700 / 1000: loss 1.965156\n",
      "iteration 800 / 1000: loss 2.081666\n",
      "iteration 900 / 1000: loss 1.909304\n",
      "training accuracy: 0.3621428571\n",
      "validation accuracy: 0.3670000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000010000\n",
      "regularization_strengths: 15000.0000000000\n",
      "iteration 0 / 1000: loss 235.174874\n",
      "iteration 100 / 1000: loss 13.102829\n",
      "iteration 200 / 1000: loss 2.526821\n",
      "iteration 300 / 1000: loss 1.929189\n",
      "iteration 400 / 1000: loss 1.972240\n",
      "iteration 500 / 1000: loss 1.945894\n",
      "iteration 600 / 1000: loss 1.939675\n",
      "iteration 700 / 1000: loss 2.084654\n",
      "iteration 800 / 1000: loss 2.019961\n",
      "iteration 900 / 1000: loss 1.991174\n",
      "training accuracy: 0.3551224490\n",
      "validation accuracy: 0.3660000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000010000\n",
      "regularization_strengths: 16000.0000000000\n",
      "iteration 0 / 1000: loss 251.096609\n",
      "iteration 100 / 1000: loss 11.586548\n",
      "iteration 200 / 1000: loss 2.400554\n",
      "iteration 300 / 1000: loss 2.040110\n",
      "iteration 400 / 1000: loss 1.988755\n",
      "iteration 500 / 1000: loss 1.945716\n",
      "iteration 600 / 1000: loss 1.950394\n",
      "iteration 700 / 1000: loss 2.031572\n",
      "iteration 800 / 1000: loss 1.990671\n",
      "iteration 900 / 1000: loss 1.929789\n",
      "training accuracy: 0.3605714286\n",
      "validation accuracy: 0.3650000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000010000\n",
      "regularization_strengths: 17000.0000000000\n",
      "iteration 0 / 1000: loss 264.130553\n",
      "iteration 100 / 1000: loss 10.250116\n",
      "iteration 200 / 1000: loss 2.269604\n",
      "iteration 300 / 1000: loss 1.968835\n",
      "iteration 400 / 1000: loss 1.955601\n",
      "iteration 500 / 1000: loss 2.037779\n",
      "iteration 600 / 1000: loss 1.927350\n",
      "iteration 700 / 1000: loss 1.907210\n",
      "iteration 800 / 1000: loss 1.915012\n",
      "iteration 900 / 1000: loss 2.038329\n",
      "training accuracy: 0.3550000000\n",
      "validation accuracy: 0.3700000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000010000\n",
      "regularization_strengths: 18000.0000000000\n",
      "iteration 0 / 1000: loss 282.719994\n",
      "iteration 100 / 1000: loss 9.199016\n",
      "iteration 200 / 1000: loss 2.162097\n",
      "iteration 300 / 1000: loss 2.040333\n",
      "iteration 400 / 1000: loss 2.014970\n",
      "iteration 500 / 1000: loss 2.057959\n",
      "iteration 600 / 1000: loss 1.941299\n",
      "iteration 700 / 1000: loss 1.978418\n",
      "iteration 800 / 1000: loss 2.002467\n",
      "iteration 900 / 1000: loss 2.071492\n",
      "training accuracy: 0.3491428571\n",
      "validation accuracy: 0.3410000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000010000\n",
      "regularization_strengths: 19000.0000000000\n",
      "iteration 0 / 1000: loss 298.220121\n",
      "iteration 100 / 1000: loss 8.286392\n",
      "iteration 200 / 1000: loss 2.140137\n",
      "iteration 300 / 1000: loss 1.974163\n",
      "iteration 400 / 1000: loss 1.946059\n",
      "iteration 500 / 1000: loss 2.104951\n",
      "iteration 600 / 1000: loss 1.987663\n",
      "iteration 700 / 1000: loss 1.980744\n",
      "iteration 800 / 1000: loss 2.017185\n",
      "iteration 900 / 1000: loss 1.943451\n",
      "training accuracy: 0.3477346939\n",
      "validation accuracy: 0.3670000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000010000\n",
      "regularization_strengths: 20000.0000000000\n",
      "iteration 0 / 1000: loss 312.281778\n",
      "iteration 100 / 1000: loss 7.238925\n",
      "iteration 200 / 1000: loss 2.149655\n",
      "iteration 300 / 1000: loss 2.084535\n",
      "iteration 400 / 1000: loss 2.060687\n",
      "iteration 500 / 1000: loss 2.046352\n",
      "iteration 600 / 1000: loss 2.012918\n",
      "iteration 700 / 1000: loss 1.971316\n",
      "iteration 800 / 1000: loss 2.032823\n",
      "iteration 900 / 1000: loss 2.061017\n",
      "training accuracy: 0.3498571429\n",
      "validation accuracy: 0.3550000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000010000\n",
      "regularization_strengths: 21000.0000000000\n",
      "iteration 0 / 1000: loss 324.365525\n",
      "iteration 100 / 1000: loss 6.469251\n",
      "iteration 200 / 1000: loss 2.129651\n",
      "iteration 300 / 1000: loss 1.975379\n",
      "iteration 400 / 1000: loss 1.994296\n",
      "iteration 500 / 1000: loss 2.048366\n",
      "iteration 600 / 1000: loss 2.003291\n",
      "iteration 700 / 1000: loss 2.049226\n",
      "iteration 800 / 1000: loss 2.023713\n",
      "iteration 900 / 1000: loss 2.066190\n",
      "training accuracy: 0.3476530612\n",
      "validation accuracy: 0.3570000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000010000\n",
      "regularization_strengths: 22000.0000000000\n",
      "iteration 0 / 1000: loss 339.081084\n",
      "iteration 100 / 1000: loss 5.859025\n",
      "iteration 200 / 1000: loss 2.007287\n",
      "iteration 300 / 1000: loss 1.999986\n",
      "iteration 400 / 1000: loss 2.095134\n",
      "iteration 500 / 1000: loss 1.965539\n",
      "iteration 600 / 1000: loss 2.021716\n",
      "iteration 700 / 1000: loss 2.095254\n",
      "iteration 800 / 1000: loss 1.982736\n",
      "iteration 900 / 1000: loss 2.110913\n",
      "training accuracy: 0.3502653061\n",
      "validation accuracy: 0.3520000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000010000\n",
      "regularization_strengths: 23000.0000000000\n",
      "iteration 0 / 1000: loss 359.995477\n",
      "iteration 100 / 1000: loss 5.336349\n",
      "iteration 200 / 1000: loss 2.123572\n",
      "iteration 300 / 1000: loss 1.972174\n",
      "iteration 400 / 1000: loss 1.982971\n",
      "iteration 500 / 1000: loss 2.000568\n",
      "iteration 600 / 1000: loss 2.009839\n",
      "iteration 700 / 1000: loss 2.031683\n",
      "iteration 800 / 1000: loss 2.021041\n",
      "iteration 900 / 1000: loss 2.039550\n",
      "training accuracy: 0.3553877551\n",
      "validation accuracy: 0.3690000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000010000\n",
      "regularization_strengths: 24000.0000000000\n",
      "iteration 0 / 1000: loss 371.464297\n",
      "iteration 100 / 1000: loss 4.797857\n",
      "iteration 200 / 1000: loss 2.100157\n",
      "iteration 300 / 1000: loss 2.024602\n",
      "iteration 400 / 1000: loss 2.081932\n",
      "iteration 500 / 1000: loss 2.079228\n",
      "iteration 600 / 1000: loss 2.091995\n",
      "iteration 700 / 1000: loss 2.018875\n",
      "iteration 800 / 1000: loss 1.993524\n",
      "iteration 900 / 1000: loss 2.003633\n",
      "training accuracy: 0.3428367347\n",
      "validation accuracy: 0.3420000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000010000\n",
      "regularization_strengths: 25000.0000000000\n",
      "iteration 0 / 1000: loss 392.908889\n",
      "iteration 100 / 1000: loss 4.401180\n",
      "iteration 200 / 1000: loss 1.945078\n",
      "iteration 300 / 1000: loss 2.080415\n",
      "iteration 400 / 1000: loss 2.042560\n",
      "iteration 500 / 1000: loss 2.039602\n",
      "iteration 600 / 1000: loss 1.962196\n",
      "iteration 700 / 1000: loss 2.029496\n",
      "iteration 800 / 1000: loss 1.922191\n",
      "iteration 900 / 1000: loss 2.032647\n",
      "training accuracy: 0.3448163265\n",
      "validation accuracy: 0.3430000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000010000\n",
      "regularization_strengths: 30000.0000000000\n",
      "iteration 0 / 1000: loss 465.121533\n",
      "iteration 100 / 1000: loss 3.066965\n",
      "iteration 200 / 1000: loss 2.051958\n",
      "iteration 300 / 1000: loss 2.108485\n",
      "iteration 400 / 1000: loss 2.072599\n",
      "iteration 500 / 1000: loss 2.048968\n",
      "iteration 600 / 1000: loss 2.095497\n",
      "iteration 700 / 1000: loss 2.077719\n",
      "iteration 800 / 1000: loss 2.030122\n",
      "iteration 900 / 1000: loss 2.107238\n",
      "training accuracy: 0.3357551020\n",
      "validation accuracy: 0.3440000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000010000\n",
      "regularization_strengths: 40000.0000000000\n",
      "iteration 0 / 1000: loss 617.158263\n",
      "iteration 100 / 1000: loss 2.276483\n",
      "iteration 200 / 1000: loss 2.070762\n",
      "iteration 300 / 1000: loss 2.131639\n",
      "iteration 400 / 1000: loss 2.076508\n",
      "iteration 500 / 1000: loss 2.062254\n",
      "iteration 600 / 1000: loss 2.065955\n",
      "iteration 700 / 1000: loss 2.086690\n",
      "iteration 800 / 1000: loss 2.092856\n",
      "iteration 900 / 1000: loss 2.016558\n",
      "training accuracy: 0.3373673469\n",
      "validation accuracy: 0.3480000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000040000\n",
      "regularization_strengths: 5000.0000000000\n",
      "iteration 0 / 1000: loss 82.839819\n",
      "iteration 100 / 1000: loss 3.444922\n",
      "iteration 200 / 1000: loss 1.862843\n",
      "iteration 300 / 1000: loss 1.933210\n",
      "iteration 400 / 1000: loss 2.062241\n",
      "iteration 500 / 1000: loss 1.928749\n",
      "iteration 600 / 1000: loss 1.999677\n",
      "iteration 700 / 1000: loss 1.896800\n",
      "iteration 800 / 1000: loss 2.026776\n",
      "iteration 900 / 1000: loss 1.987352\n",
      "training accuracy: 0.3346938776\n",
      "validation accuracy: 0.3320000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000040000\n",
      "regularization_strengths: 10000.0000000000\n",
      "iteration 0 / 1000: loss 162.084474\n",
      "iteration 100 / 1000: loss 2.114910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 200 / 1000: loss 2.151513\n",
      "iteration 300 / 1000: loss 2.012125\n",
      "iteration 400 / 1000: loss 2.146123\n",
      "iteration 500 / 1000: loss 2.130010\n",
      "iteration 600 / 1000: loss 2.053469\n",
      "iteration 700 / 1000: loss 1.962029\n",
      "iteration 800 / 1000: loss 2.062614\n",
      "iteration 900 / 1000: loss 2.046752\n",
      "training accuracy: 0.3288163265\n",
      "validation accuracy: 0.3310000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000040000\n",
      "regularization_strengths: 11000.0000000000\n",
      "iteration 0 / 1000: loss 176.177257\n",
      "iteration 100 / 1000: loss 2.057204\n",
      "iteration 200 / 1000: loss 1.955396\n",
      "iteration 300 / 1000: loss 2.088225\n",
      "iteration 400 / 1000: loss 1.938602\n",
      "iteration 500 / 1000: loss 2.026935\n",
      "iteration 600 / 1000: loss 2.074141\n",
      "iteration 700 / 1000: loss 1.968177\n",
      "iteration 800 / 1000: loss 2.042615\n",
      "iteration 900 / 1000: loss 2.012003\n",
      "training accuracy: 0.3166122449\n",
      "validation accuracy: 0.3320000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000040000\n",
      "regularization_strengths: 12000.0000000000\n",
      "iteration 0 / 1000: loss 192.187208\n",
      "iteration 100 / 1000: loss 2.057403\n",
      "iteration 200 / 1000: loss 1.943094\n",
      "iteration 300 / 1000: loss 2.003752\n",
      "iteration 400 / 1000: loss 1.944599\n",
      "iteration 500 / 1000: loss 2.000464\n",
      "iteration 600 / 1000: loss 2.006001\n",
      "iteration 700 / 1000: loss 2.005915\n",
      "iteration 800 / 1000: loss 1.921112\n",
      "iteration 900 / 1000: loss 2.047506\n",
      "training accuracy: 0.3307551020\n",
      "validation accuracy: 0.3330000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000040000\n",
      "regularization_strengths: 13000.0000000000\n",
      "iteration 0 / 1000: loss 207.220235\n",
      "iteration 100 / 1000: loss 1.949720\n",
      "iteration 200 / 1000: loss 2.133110\n",
      "iteration 300 / 1000: loss 2.091805\n",
      "iteration 400 / 1000: loss 1.961992\n",
      "iteration 500 / 1000: loss 2.059512\n",
      "iteration 600 / 1000: loss 2.021142\n",
      "iteration 700 / 1000: loss 2.028000\n",
      "iteration 800 / 1000: loss 2.000119\n",
      "iteration 900 / 1000: loss 2.039732\n",
      "training accuracy: 0.3145510204\n",
      "validation accuracy: 0.3210000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000040000\n",
      "regularization_strengths: 14000.0000000000\n",
      "iteration 0 / 1000: loss 223.092469\n",
      "iteration 100 / 1000: loss 2.003050\n",
      "iteration 200 / 1000: loss 2.059818\n",
      "iteration 300 / 1000: loss 2.068610\n",
      "iteration 400 / 1000: loss 2.068094\n",
      "iteration 500 / 1000: loss 2.007889\n",
      "iteration 600 / 1000: loss 1.944977\n",
      "iteration 700 / 1000: loss 2.000218\n",
      "iteration 800 / 1000: loss 2.062682\n",
      "iteration 900 / 1000: loss 2.067450\n",
      "training accuracy: 0.3337551020\n",
      "validation accuracy: 0.3280000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000040000\n",
      "regularization_strengths: 15000.0000000000\n",
      "iteration 0 / 1000: loss 236.380003\n",
      "iteration 100 / 1000: loss 2.127864\n",
      "iteration 200 / 1000: loss 2.130324\n",
      "iteration 300 / 1000: loss 2.021798\n",
      "iteration 400 / 1000: loss 2.038135\n",
      "iteration 500 / 1000: loss 2.088393\n",
      "iteration 600 / 1000: loss 2.073842\n",
      "iteration 700 / 1000: loss 2.023681\n",
      "iteration 800 / 1000: loss 1.993428\n",
      "iteration 900 / 1000: loss 2.105045\n",
      "training accuracy: 0.2783061224\n",
      "validation accuracy: 0.2840000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000040000\n",
      "regularization_strengths: 16000.0000000000\n",
      "iteration 0 / 1000: loss 251.202679\n",
      "iteration 100 / 1000: loss 1.957841\n",
      "iteration 200 / 1000: loss 2.068231\n",
      "iteration 300 / 1000: loss 2.050716\n",
      "iteration 400 / 1000: loss 2.050589\n",
      "iteration 500 / 1000: loss 2.046313\n",
      "iteration 600 / 1000: loss 1.965811\n",
      "iteration 700 / 1000: loss 2.003301\n",
      "iteration 800 / 1000: loss 2.108209\n",
      "iteration 900 / 1000: loss 2.153696\n",
      "training accuracy: 0.3209795918\n",
      "validation accuracy: 0.3060000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000040000\n",
      "regularization_strengths: 17000.0000000000\n",
      "iteration 0 / 1000: loss 267.233944\n",
      "iteration 100 / 1000: loss 2.026482\n",
      "iteration 200 / 1000: loss 2.031497\n",
      "iteration 300 / 1000: loss 2.155693\n",
      "iteration 400 / 1000: loss 2.140560\n",
      "iteration 500 / 1000: loss 2.008889\n",
      "iteration 600 / 1000: loss 2.005711\n",
      "iteration 700 / 1000: loss 2.095281\n",
      "iteration 800 / 1000: loss 2.088807\n",
      "iteration 900 / 1000: loss 2.186339\n",
      "training accuracy: 0.2971224490\n",
      "validation accuracy: 0.3230000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000040000\n",
      "regularization_strengths: 18000.0000000000\n",
      "iteration 0 / 1000: loss 280.159269\n",
      "iteration 100 / 1000: loss 2.034667\n",
      "iteration 200 / 1000: loss 2.242141\n",
      "iteration 300 / 1000: loss 2.172471\n",
      "iteration 400 / 1000: loss 2.024497\n",
      "iteration 500 / 1000: loss 2.146774\n",
      "iteration 600 / 1000: loss 2.116437\n",
      "iteration 700 / 1000: loss 2.146845\n",
      "iteration 800 / 1000: loss 2.059353\n",
      "iteration 900 / 1000: loss 2.089965\n",
      "training accuracy: 0.3201428571\n",
      "validation accuracy: 0.3210000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000040000\n",
      "regularization_strengths: 19000.0000000000\n",
      "iteration 0 / 1000: loss 299.901957\n",
      "iteration 100 / 1000: loss 2.022794\n",
      "iteration 200 / 1000: loss 2.016838\n",
      "iteration 300 / 1000: loss 2.061662\n",
      "iteration 400 / 1000: loss 2.070794\n",
      "iteration 500 / 1000: loss 2.179503\n",
      "iteration 600 / 1000: loss 2.053568\n",
      "iteration 700 / 1000: loss 2.059507\n",
      "iteration 800 / 1000: loss 2.067180\n",
      "iteration 900 / 1000: loss 2.137709\n",
      "training accuracy: 0.2912448980\n",
      "validation accuracy: 0.3160000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000040000\n",
      "regularization_strengths: 20000.0000000000\n",
      "iteration 0 / 1000: loss 311.321306\n",
      "iteration 100 / 1000: loss 2.076858\n",
      "iteration 200 / 1000: loss 2.141175\n",
      "iteration 300 / 1000: loss 2.063879\n",
      "iteration 400 / 1000: loss 2.082203\n",
      "iteration 500 / 1000: loss 2.044260\n",
      "iteration 600 / 1000: loss 2.132540\n",
      "iteration 700 / 1000: loss 2.055585\n",
      "iteration 800 / 1000: loss 2.080971\n",
      "iteration 900 / 1000: loss 2.008007\n",
      "training accuracy: 0.2816122449\n",
      "validation accuracy: 0.2930000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000040000\n",
      "regularization_strengths: 21000.0000000000\n",
      "iteration 0 / 1000: loss 327.095968\n",
      "iteration 100 / 1000: loss 2.034180\n",
      "iteration 200 / 1000: loss 2.108260\n",
      "iteration 300 / 1000: loss 2.113020\n",
      "iteration 400 / 1000: loss 2.099262\n",
      "iteration 500 / 1000: loss 2.083845\n",
      "iteration 600 / 1000: loss 2.093758\n",
      "iteration 700 / 1000: loss 2.007521\n",
      "iteration 800 / 1000: loss 2.109654\n",
      "iteration 900 / 1000: loss 2.004466\n",
      "training accuracy: 0.2620816327\n",
      "validation accuracy: 0.2680000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000040000\n",
      "regularization_strengths: 22000.0000000000\n",
      "iteration 0 / 1000: loss 342.572990\n",
      "iteration 100 / 1000: loss 2.170015\n",
      "iteration 200 / 1000: loss 2.056803\n",
      "iteration 300 / 1000: loss 2.069314\n",
      "iteration 400 / 1000: loss 2.101968\n",
      "iteration 500 / 1000: loss 2.113858\n",
      "iteration 600 / 1000: loss 2.129670\n",
      "iteration 700 / 1000: loss 2.130397\n",
      "iteration 800 / 1000: loss 2.071498\n",
      "iteration 900 / 1000: loss 2.015822\n",
      "training accuracy: 0.3000000000\n",
      "validation accuracy: 0.3100000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000040000\n",
      "regularization_strengths: 23000.0000000000\n",
      "iteration 0 / 1000: loss 358.491675\n",
      "iteration 100 / 1000: loss 2.059578\n",
      "iteration 200 / 1000: loss 1.979878\n",
      "iteration 300 / 1000: loss 2.101547\n",
      "iteration 400 / 1000: loss 2.143965\n",
      "iteration 500 / 1000: loss 2.099390\n",
      "iteration 600 / 1000: loss 2.070565\n",
      "iteration 700 / 1000: loss 2.150325\n",
      "iteration 800 / 1000: loss 2.039603\n",
      "iteration 900 / 1000: loss 2.110372\n",
      "training accuracy: 0.2899183673\n",
      "validation accuracy: 0.3020000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000040000\n",
      "regularization_strengths: 24000.0000000000\n",
      "iteration 0 / 1000: loss 376.729404\n",
      "iteration 100 / 1000: loss 2.169268\n",
      "iteration 200 / 1000: loss 2.151366\n",
      "iteration 300 / 1000: loss 2.217505\n",
      "iteration 400 / 1000: loss 2.217563\n",
      "iteration 500 / 1000: loss 2.110764\n",
      "iteration 600 / 1000: loss 2.124503\n",
      "iteration 700 / 1000: loss 2.071446\n",
      "iteration 800 / 1000: loss 2.012039\n",
      "iteration 900 / 1000: loss 2.236634\n",
      "training accuracy: 0.2802857143\n",
      "validation accuracy: 0.3040000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000040000\n",
      "regularization_strengths: 25000.0000000000\n",
      "iteration 0 / 1000: loss 393.121396\n",
      "iteration 100 / 1000: loss 2.115390\n",
      "iteration 200 / 1000: loss 2.041397\n",
      "iteration 300 / 1000: loss 2.145930\n",
      "iteration 400 / 1000: loss 2.153733\n",
      "iteration 500 / 1000: loss 2.096167\n",
      "iteration 600 / 1000: loss 2.181813\n",
      "iteration 700 / 1000: loss 2.256847\n",
      "iteration 800 / 1000: loss 2.144815\n",
      "iteration 900 / 1000: loss 2.183349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.2867142857\n",
      "validation accuracy: 0.2980000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000040000\n",
      "regularization_strengths: 30000.0000000000\n",
      "iteration 0 / 1000: loss 471.106592\n",
      "iteration 100 / 1000: loss 2.108848\n",
      "iteration 200 / 1000: loss 2.148014\n",
      "iteration 300 / 1000: loss 2.473048\n",
      "iteration 400 / 1000: loss 2.192728\n",
      "iteration 500 / 1000: loss 2.224683\n",
      "iteration 600 / 1000: loss 2.043044\n",
      "iteration 700 / 1000: loss 2.266783\n",
      "iteration 800 / 1000: loss 2.118050\n",
      "iteration 900 / 1000: loss 2.063160\n",
      "training accuracy: 0.2751428571\n",
      "validation accuracy: 0.2800000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000040000\n",
      "regularization_strengths: 40000.0000000000\n",
      "iteration 0 / 1000: loss 619.776437\n",
      "iteration 100 / 1000: loss 2.164250\n",
      "iteration 200 / 1000: loss 2.136319\n",
      "iteration 300 / 1000: loss 2.173428\n",
      "iteration 400 / 1000: loss 2.215859\n",
      "iteration 500 / 1000: loss 2.176737\n",
      "iteration 600 / 1000: loss 2.400864\n",
      "iteration 700 / 1000: loss 2.134039\n",
      "iteration 800 / 1000: loss 2.246584\n",
      "iteration 900 / 1000: loss 2.216792\n",
      "training accuracy: 0.2950612245\n",
      "validation accuracy: 0.2900000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000080000\n",
      "regularization_strengths: 5000.0000000000\n",
      "iteration 0 / 1000: loss 82.520144\n",
      "iteration 100 / 1000: loss 2.450234\n",
      "iteration 200 / 1000: loss 2.849486\n",
      "iteration 300 / 1000: loss 3.138677\n",
      "iteration 400 / 1000: loss 3.299327\n",
      "iteration 500 / 1000: loss 3.159787\n",
      "iteration 600 / 1000: loss 2.899389\n",
      "iteration 700 / 1000: loss 3.124422\n",
      "iteration 800 / 1000: loss 3.151316\n",
      "iteration 900 / 1000: loss 3.507694\n",
      "training accuracy: 0.2239183673\n",
      "validation accuracy: 0.2120000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000080000\n",
      "regularization_strengths: 10000.0000000000\n",
      "iteration 0 / 1000: loss 160.631727\n",
      "iteration 100 / 1000: loss 2.767826\n",
      "iteration 200 / 1000: loss 3.542628\n",
      "iteration 300 / 1000: loss 3.739271\n",
      "iteration 400 / 1000: loss 3.458819\n",
      "iteration 500 / 1000: loss 3.012013\n",
      "iteration 600 / 1000: loss 3.337890\n",
      "iteration 700 / 1000: loss 3.515621\n",
      "iteration 800 / 1000: loss 3.154532\n",
      "iteration 900 / 1000: loss 3.145878\n",
      "training accuracy: 0.1980816327\n",
      "validation accuracy: 0.2030000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000080000\n",
      "regularization_strengths: 11000.0000000000\n",
      "iteration 0 / 1000: loss 176.365331\n",
      "iteration 100 / 1000: loss 4.245774\n",
      "iteration 200 / 1000: loss 3.539315\n",
      "iteration 300 / 1000: loss 2.611305\n",
      "iteration 400 / 1000: loss 3.758071\n",
      "iteration 500 / 1000: loss 2.685151\n",
      "iteration 600 / 1000: loss 5.194943\n",
      "iteration 700 / 1000: loss 3.181105\n",
      "iteration 800 / 1000: loss 3.673529\n",
      "iteration 900 / 1000: loss 3.260120\n",
      "training accuracy: 0.2165918367\n",
      "validation accuracy: 0.2270000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000080000\n",
      "regularization_strengths: 12000.0000000000\n",
      "iteration 0 / 1000: loss 190.713714\n",
      "iteration 100 / 1000: loss 2.689688\n",
      "iteration 200 / 1000: loss 4.370518\n",
      "iteration 300 / 1000: loss 2.456029\n",
      "iteration 400 / 1000: loss 2.968632\n",
      "iteration 500 / 1000: loss 3.641888\n",
      "iteration 600 / 1000: loss 3.550096\n",
      "iteration 700 / 1000: loss 2.991461\n",
      "iteration 800 / 1000: loss 4.338173\n",
      "iteration 900 / 1000: loss 2.887733\n",
      "training accuracy: 0.2298979592\n",
      "validation accuracy: 0.2420000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000080000\n",
      "regularization_strengths: 13000.0000000000\n",
      "iteration 0 / 1000: loss 207.085567\n",
      "iteration 100 / 1000: loss 4.808063\n",
      "iteration 200 / 1000: loss 2.488775\n",
      "iteration 300 / 1000: loss 3.470437\n",
      "iteration 400 / 1000: loss 5.773630\n",
      "iteration 500 / 1000: loss 3.500773\n",
      "iteration 600 / 1000: loss 3.808162\n",
      "iteration 700 / 1000: loss 3.313981\n",
      "iteration 800 / 1000: loss 2.752249\n",
      "iteration 900 / 1000: loss 4.760860\n",
      "training accuracy: 0.2509387755\n",
      "validation accuracy: 0.2600000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000080000\n",
      "regularization_strengths: 14000.0000000000\n",
      "iteration 0 / 1000: loss 222.250896\n",
      "iteration 100 / 1000: loss 3.707320\n",
      "iteration 200 / 1000: loss 3.356480\n",
      "iteration 300 / 1000: loss 2.681026\n",
      "iteration 400 / 1000: loss 4.245041\n",
      "iteration 500 / 1000: loss 3.541531\n",
      "iteration 600 / 1000: loss 2.873699\n",
      "iteration 700 / 1000: loss 3.686971\n",
      "iteration 800 / 1000: loss 4.077865\n",
      "iteration 900 / 1000: loss 3.133224\n",
      "training accuracy: 0.1921224490\n",
      "validation accuracy: 0.1850000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000080000\n",
      "regularization_strengths: 15000.0000000000\n",
      "iteration 0 / 1000: loss 237.206731\n",
      "iteration 100 / 1000: loss 2.847381\n",
      "iteration 200 / 1000: loss 2.994499\n",
      "iteration 300 / 1000: loss 3.697613\n",
      "iteration 400 / 1000: loss 3.967759\n",
      "iteration 500 / 1000: loss 2.624276\n",
      "iteration 600 / 1000: loss 3.009948\n",
      "iteration 700 / 1000: loss 3.669802\n",
      "iteration 800 / 1000: loss 3.492166\n",
      "iteration 900 / 1000: loss 2.761936\n",
      "training accuracy: 0.1698571429\n",
      "validation accuracy: 0.1780000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000080000\n",
      "regularization_strengths: 16000.0000000000\n",
      "iteration 0 / 1000: loss 252.079378\n",
      "iteration 100 / 1000: loss 4.018395\n",
      "iteration 200 / 1000: loss 3.155985\n",
      "iteration 300 / 1000: loss 4.360330\n",
      "iteration 400 / 1000: loss 3.446476\n",
      "iteration 500 / 1000: loss 3.955407\n",
      "iteration 600 / 1000: loss 5.462101\n",
      "iteration 700 / 1000: loss 2.748135\n",
      "iteration 800 / 1000: loss 3.834051\n",
      "iteration 900 / 1000: loss 4.653063\n",
      "training accuracy: 0.2408775510\n",
      "validation accuracy: 0.2380000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000080000\n",
      "regularization_strengths: 17000.0000000000\n",
      "iteration 0 / 1000: loss 266.114795\n",
      "iteration 100 / 1000: loss 3.477039\n",
      "iteration 200 / 1000: loss 3.434465\n",
      "iteration 300 / 1000: loss 3.593113\n",
      "iteration 400 / 1000: loss 3.233828\n",
      "iteration 500 / 1000: loss 3.953998\n",
      "iteration 600 / 1000: loss 4.963870\n",
      "iteration 700 / 1000: loss 4.405737\n",
      "iteration 800 / 1000: loss 3.423053\n",
      "iteration 900 / 1000: loss 3.321354\n",
      "training accuracy: 0.2346326531\n",
      "validation accuracy: 0.2330000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000080000\n",
      "regularization_strengths: 18000.0000000000\n",
      "iteration 0 / 1000: loss 277.895452\n",
      "iteration 100 / 1000: loss 2.641457\n",
      "iteration 200 / 1000: loss 3.592771\n",
      "iteration 300 / 1000: loss 3.142809\n",
      "iteration 400 / 1000: loss 3.591419\n",
      "iteration 500 / 1000: loss 3.813542\n",
      "iteration 600 / 1000: loss 4.498320\n",
      "iteration 700 / 1000: loss 4.143391\n",
      "iteration 800 / 1000: loss 3.100817\n",
      "iteration 900 / 1000: loss 3.415254\n",
      "training accuracy: 0.2419387755\n",
      "validation accuracy: 0.2530000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000080000\n",
      "regularization_strengths: 19000.0000000000\n",
      "iteration 0 / 1000: loss 299.010614\n",
      "iteration 100 / 1000: loss 4.790741\n",
      "iteration 200 / 1000: loss 4.415957\n",
      "iteration 300 / 1000: loss 4.144271\n",
      "iteration 400 / 1000: loss 3.929129\n",
      "iteration 500 / 1000: loss 4.516340\n",
      "iteration 600 / 1000: loss 3.036229\n",
      "iteration 700 / 1000: loss 3.718209\n",
      "iteration 800 / 1000: loss 4.738611\n",
      "iteration 900 / 1000: loss 3.510398\n",
      "training accuracy: 0.2502040816\n",
      "validation accuracy: 0.2440000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000080000\n",
      "regularization_strengths: 20000.0000000000\n",
      "iteration 0 / 1000: loss 314.034801\n",
      "iteration 100 / 1000: loss 2.977779\n",
      "iteration 200 / 1000: loss 3.061894\n",
      "iteration 300 / 1000: loss 4.719896\n",
      "iteration 400 / 1000: loss 4.246208\n",
      "iteration 500 / 1000: loss 4.075275\n",
      "iteration 600 / 1000: loss 3.137425\n",
      "iteration 700 / 1000: loss 3.441772\n",
      "iteration 800 / 1000: loss 2.973001\n",
      "iteration 900 / 1000: loss 4.631453\n",
      "training accuracy: 0.2248775510\n",
      "validation accuracy: 0.2490000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000080000\n",
      "regularization_strengths: 21000.0000000000\n",
      "iteration 0 / 1000: loss 325.550788\n",
      "iteration 100 / 1000: loss 4.587415\n",
      "iteration 200 / 1000: loss 2.869204\n",
      "iteration 300 / 1000: loss 3.178663\n",
      "iteration 400 / 1000: loss 4.108340\n",
      "iteration 500 / 1000: loss 3.802867\n",
      "iteration 600 / 1000: loss 5.302395\n",
      "iteration 700 / 1000: loss 3.238275\n",
      "iteration 800 / 1000: loss 4.280697\n",
      "iteration 900 / 1000: loss 4.504106\n",
      "training accuracy: 0.1640204082\n",
      "validation accuracy: 0.1690000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000080000\n",
      "regularization_strengths: 22000.0000000000\n",
      "iteration 0 / 1000: loss 342.846138\n",
      "iteration 100 / 1000: loss 4.553090\n",
      "iteration 200 / 1000: loss 4.529285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 300 / 1000: loss 3.373423\n",
      "iteration 400 / 1000: loss 4.005434\n",
      "iteration 500 / 1000: loss 3.752898\n",
      "iteration 600 / 1000: loss 4.278333\n",
      "iteration 700 / 1000: loss 3.577651\n",
      "iteration 800 / 1000: loss 3.959333\n",
      "iteration 900 / 1000: loss 3.933633\n",
      "training accuracy: 0.1760000000\n",
      "validation accuracy: 0.1940000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000080000\n",
      "regularization_strengths: 23000.0000000000\n",
      "iteration 0 / 1000: loss 360.425822\n",
      "iteration 100 / 1000: loss 4.043077\n",
      "iteration 200 / 1000: loss 4.512067\n",
      "iteration 300 / 1000: loss 3.730043\n",
      "iteration 400 / 1000: loss 2.777903\n",
      "iteration 500 / 1000: loss 3.810502\n",
      "iteration 600 / 1000: loss 3.878998\n",
      "iteration 700 / 1000: loss 4.068817\n",
      "iteration 800 / 1000: loss 2.678442\n",
      "iteration 900 / 1000: loss 3.081879\n",
      "training accuracy: 0.2021428571\n",
      "validation accuracy: 0.2170000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000080000\n",
      "regularization_strengths: 24000.0000000000\n",
      "iteration 0 / 1000: loss 374.963380\n",
      "iteration 100 / 1000: loss 3.507692\n",
      "iteration 200 / 1000: loss 3.579001\n",
      "iteration 300 / 1000: loss 4.229925\n",
      "iteration 400 / 1000: loss 3.920567\n",
      "iteration 500 / 1000: loss 3.710513\n",
      "iteration 600 / 1000: loss 3.935416\n",
      "iteration 700 / 1000: loss 4.096614\n",
      "iteration 800 / 1000: loss 4.834196\n",
      "iteration 900 / 1000: loss 3.770385\n",
      "training accuracy: 0.1511836735\n",
      "validation accuracy: 0.1650000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000080000\n",
      "regularization_strengths: 25000.0000000000\n",
      "iteration 0 / 1000: loss 388.072560\n",
      "iteration 100 / 1000: loss 6.131595\n",
      "iteration 200 / 1000: loss 3.372277\n",
      "iteration 300 / 1000: loss 4.497235\n",
      "iteration 400 / 1000: loss 3.488314\n",
      "iteration 500 / 1000: loss 3.317014\n",
      "iteration 600 / 1000: loss 4.429815\n",
      "iteration 700 / 1000: loss 2.835970\n",
      "iteration 800 / 1000: loss 2.964501\n",
      "iteration 900 / 1000: loss 4.615418\n",
      "training accuracy: 0.1952653061\n",
      "validation accuracy: 0.1710000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000080000\n",
      "regularization_strengths: 30000.0000000000\n",
      "iteration 0 / 1000: loss 463.339904\n",
      "iteration 100 / 1000: loss 4.416781\n",
      "iteration 200 / 1000: loss 4.462039\n",
      "iteration 300 / 1000: loss 4.921213\n",
      "iteration 400 / 1000: loss 5.027633\n",
      "iteration 500 / 1000: loss 4.043409\n",
      "iteration 600 / 1000: loss 4.494196\n",
      "iteration 700 / 1000: loss 3.054167\n",
      "iteration 800 / 1000: loss 3.775269\n",
      "iteration 900 / 1000: loss 4.323439\n",
      "training accuracy: 0.1867959184\n",
      "validation accuracy: 0.1940000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000080000\n",
      "regularization_strengths: 40000.0000000000\n",
      "iteration 0 / 1000: loss 626.512427\n",
      "iteration 100 / 1000: loss 5.998959\n",
      "iteration 200 / 1000: loss 4.536644\n",
      "iteration 300 / 1000: loss 4.176388\n",
      "iteration 400 / 1000: loss 3.525865\n",
      "iteration 500 / 1000: loss 4.952774\n",
      "iteration 600 / 1000: loss 3.478396\n",
      "iteration 700 / 1000: loss 4.581651\n",
      "iteration 800 / 1000: loss 4.292373\n",
      "iteration 900 / 1000: loss 3.466547\n",
      "training accuracy: 0.1387551020\n",
      "validation accuracy: 0.1360000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000100000\n",
      "regularization_strengths: 5000.0000000000\n",
      "iteration 0 / 1000: loss 83.772705\n",
      "iteration 100 / 1000: loss 3.878633\n",
      "iteration 200 / 1000: loss 3.054221\n",
      "iteration 300 / 1000: loss 3.941801\n",
      "iteration 400 / 1000: loss 3.611506\n",
      "iteration 500 / 1000: loss 3.662826\n",
      "iteration 600 / 1000: loss 2.328024\n",
      "iteration 700 / 1000: loss 4.894348\n",
      "iteration 800 / 1000: loss 3.514869\n",
      "iteration 900 / 1000: loss 3.619338\n",
      "training accuracy: 0.2552653061\n",
      "validation accuracy: 0.2670000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000100000\n",
      "regularization_strengths: 10000.0000000000\n",
      "iteration 0 / 1000: loss 157.647462\n",
      "iteration 100 / 1000: loss 5.174300\n",
      "iteration 200 / 1000: loss 3.155305\n",
      "iteration 300 / 1000: loss 3.326970\n",
      "iteration 400 / 1000: loss 4.173988\n",
      "iteration 500 / 1000: loss 3.144324\n",
      "iteration 600 / 1000: loss 5.394778\n",
      "iteration 700 / 1000: loss 4.507204\n",
      "iteration 800 / 1000: loss 5.020040\n",
      "iteration 900 / 1000: loss 4.665066\n",
      "training accuracy: 0.2315510204\n",
      "validation accuracy: 0.2390000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000100000\n",
      "regularization_strengths: 11000.0000000000\n",
      "iteration 0 / 1000: loss 175.482543\n",
      "iteration 100 / 1000: loss 2.787981\n",
      "iteration 200 / 1000: loss 3.597863\n",
      "iteration 300 / 1000: loss 4.569459\n",
      "iteration 400 / 1000: loss 5.655303\n",
      "iteration 500 / 1000: loss 4.520833\n",
      "iteration 600 / 1000: loss 3.590046\n",
      "iteration 700 / 1000: loss 4.429060\n",
      "iteration 800 / 1000: loss 3.636363\n",
      "iteration 900 / 1000: loss 4.563588\n",
      "training accuracy: 0.1896938776\n",
      "validation accuracy: 0.2020000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000100000\n",
      "regularization_strengths: 12000.0000000000\n",
      "iteration 0 / 1000: loss 189.048110\n",
      "iteration 100 / 1000: loss 2.863620\n",
      "iteration 200 / 1000: loss 2.739674\n",
      "iteration 300 / 1000: loss 3.747809\n",
      "iteration 400 / 1000: loss 4.788907\n",
      "iteration 500 / 1000: loss 3.966155\n",
      "iteration 600 / 1000: loss 7.067047\n",
      "iteration 700 / 1000: loss 3.579596\n",
      "iteration 800 / 1000: loss 4.897524\n",
      "iteration 900 / 1000: loss 5.768418\n",
      "training accuracy: 0.2128571429\n",
      "validation accuracy: 0.2260000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000100000\n",
      "regularization_strengths: 13000.0000000000\n",
      "iteration 0 / 1000: loss 203.007194\n",
      "iteration 100 / 1000: loss 5.382330\n",
      "iteration 200 / 1000: loss 3.585219\n",
      "iteration 300 / 1000: loss 4.550825\n",
      "iteration 400 / 1000: loss 3.860853\n",
      "iteration 500 / 1000: loss 3.988829\n",
      "iteration 600 / 1000: loss 4.177955\n",
      "iteration 700 / 1000: loss 3.548108\n",
      "iteration 800 / 1000: loss 3.777447\n",
      "iteration 900 / 1000: loss 4.222744\n",
      "training accuracy: 0.2037959184\n",
      "validation accuracy: 0.2040000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000100000\n",
      "regularization_strengths: 14000.0000000000\n",
      "iteration 0 / 1000: loss 220.119393\n",
      "iteration 100 / 1000: loss 3.010532\n",
      "iteration 200 / 1000: loss 4.114539\n",
      "iteration 300 / 1000: loss 4.164999\n",
      "iteration 400 / 1000: loss 3.887628\n",
      "iteration 500 / 1000: loss 3.539871\n",
      "iteration 600 / 1000: loss 4.837722\n",
      "iteration 700 / 1000: loss 4.845979\n",
      "iteration 800 / 1000: loss 2.979899\n",
      "iteration 900 / 1000: loss 3.622618\n",
      "training accuracy: 0.1862040816\n",
      "validation accuracy: 0.1890000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000100000\n",
      "regularization_strengths: 15000.0000000000\n",
      "iteration 0 / 1000: loss 237.751385\n",
      "iteration 100 / 1000: loss 4.374110\n",
      "iteration 200 / 1000: loss 3.779171\n",
      "iteration 300 / 1000: loss 3.608334\n",
      "iteration 400 / 1000: loss 4.271154\n",
      "iteration 500 / 1000: loss 6.812695\n",
      "iteration 600 / 1000: loss 4.290867\n",
      "iteration 700 / 1000: loss 5.755644\n",
      "iteration 800 / 1000: loss 4.010028\n",
      "iteration 900 / 1000: loss 5.564008\n",
      "training accuracy: 0.1864081633\n",
      "validation accuracy: 0.2060000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000100000\n",
      "regularization_strengths: 16000.0000000000\n",
      "iteration 0 / 1000: loss 255.022273\n",
      "iteration 100 / 1000: loss 4.923048\n",
      "iteration 200 / 1000: loss 4.340005\n",
      "iteration 300 / 1000: loss 5.015664\n",
      "iteration 400 / 1000: loss 6.049711\n",
      "iteration 500 / 1000: loss 4.717709\n",
      "iteration 600 / 1000: loss 5.282866\n",
      "iteration 700 / 1000: loss 4.079563\n",
      "iteration 800 / 1000: loss 5.104946\n",
      "iteration 900 / 1000: loss 6.148521\n",
      "training accuracy: 0.1780612245\n",
      "validation accuracy: 0.1960000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000100000\n",
      "regularization_strengths: 17000.0000000000\n",
      "iteration 0 / 1000: loss 265.116590\n",
      "iteration 100 / 1000: loss 5.222403\n",
      "iteration 200 / 1000: loss 3.235303\n",
      "iteration 300 / 1000: loss 4.547404\n",
      "iteration 400 / 1000: loss 4.962914\n",
      "iteration 500 / 1000: loss 4.404685\n",
      "iteration 600 / 1000: loss 6.316116\n",
      "iteration 700 / 1000: loss 5.112955\n",
      "iteration 800 / 1000: loss 5.287437\n",
      "iteration 900 / 1000: loss 3.617079\n",
      "training accuracy: 0.1623265306\n",
      "validation accuracy: 0.1640000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000100000\n",
      "regularization_strengths: 18000.0000000000\n",
      "iteration 0 / 1000: loss 282.957404\n",
      "iteration 100 / 1000: loss 3.485583\n",
      "iteration 200 / 1000: loss 6.469967\n",
      "iteration 300 / 1000: loss 6.071515\n",
      "iteration 400 / 1000: loss 2.610618\n",
      "iteration 500 / 1000: loss 6.898583\n",
      "iteration 600 / 1000: loss 2.884744\n",
      "iteration 700 / 1000: loss 5.973976\n",
      "iteration 800 / 1000: loss 3.992473\n",
      "iteration 900 / 1000: loss 6.174561\n",
      "training accuracy: 0.2527755102\n",
      "validation accuracy: 0.2470000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000100000\n",
      "regularization_strengths: 19000.0000000000\n",
      "iteration 0 / 1000: loss 296.968372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 1000: loss 4.948472\n",
      "iteration 200 / 1000: loss 3.790286\n",
      "iteration 300 / 1000: loss 5.514206\n",
      "iteration 400 / 1000: loss 5.769794\n",
      "iteration 500 / 1000: loss 6.327070\n",
      "iteration 600 / 1000: loss 4.952430\n",
      "iteration 700 / 1000: loss 3.888341\n",
      "iteration 800 / 1000: loss 5.556946\n",
      "iteration 900 / 1000: loss 5.842220\n",
      "training accuracy: 0.1657346939\n",
      "validation accuracy: 0.1810000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000100000\n",
      "regularization_strengths: 20000.0000000000\n",
      "iteration 0 / 1000: loss 310.493573\n",
      "iteration 100 / 1000: loss 6.115711\n",
      "iteration 200 / 1000: loss 5.374214\n",
      "iteration 300 / 1000: loss 4.543652\n",
      "iteration 400 / 1000: loss 4.291266\n",
      "iteration 500 / 1000: loss 5.950699\n",
      "iteration 600 / 1000: loss 4.819503\n",
      "iteration 700 / 1000: loss 4.406888\n",
      "iteration 800 / 1000: loss 5.023982\n",
      "iteration 900 / 1000: loss 5.741937\n",
      "training accuracy: 0.2008163265\n",
      "validation accuracy: 0.2020000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000100000\n",
      "regularization_strengths: 21000.0000000000\n",
      "iteration 0 / 1000: loss 329.783689\n",
      "iteration 100 / 1000: loss 3.748312\n",
      "iteration 200 / 1000: loss 4.791237\n",
      "iteration 300 / 1000: loss 4.341018\n",
      "iteration 400 / 1000: loss 5.000347\n",
      "iteration 500 / 1000: loss 6.216541\n",
      "iteration 600 / 1000: loss 5.078588\n",
      "iteration 700 / 1000: loss 5.984859\n",
      "iteration 800 / 1000: loss 5.134723\n",
      "iteration 900 / 1000: loss 5.060915\n",
      "training accuracy: 0.1198367347\n",
      "validation accuracy: 0.1040000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000100000\n",
      "regularization_strengths: 22000.0000000000\n",
      "iteration 0 / 1000: loss 346.720715\n",
      "iteration 100 / 1000: loss 5.658967\n",
      "iteration 200 / 1000: loss 5.607277\n",
      "iteration 300 / 1000: loss 6.067768\n",
      "iteration 400 / 1000: loss 4.417100\n",
      "iteration 500 / 1000: loss 6.004595\n",
      "iteration 600 / 1000: loss 6.006130\n",
      "iteration 700 / 1000: loss 5.128664\n",
      "iteration 800 / 1000: loss 5.658455\n",
      "iteration 900 / 1000: loss 5.595375\n",
      "training accuracy: 0.1891836735\n",
      "validation accuracy: 0.1950000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000100000\n",
      "regularization_strengths: 23000.0000000000\n",
      "iteration 0 / 1000: loss 362.819336\n",
      "iteration 100 / 1000: loss 5.018786\n",
      "iteration 200 / 1000: loss 4.851456\n",
      "iteration 300 / 1000: loss 4.676730\n",
      "iteration 400 / 1000: loss 3.877484\n",
      "iteration 500 / 1000: loss 7.335596\n",
      "iteration 600 / 1000: loss 5.759956\n",
      "iteration 700 / 1000: loss 4.573617\n",
      "iteration 800 / 1000: loss 6.292331\n",
      "iteration 900 / 1000: loss 5.635794\n",
      "training accuracy: 0.1553469388\n",
      "validation accuracy: 0.1820000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000100000\n",
      "regularization_strengths: 24000.0000000000\n",
      "iteration 0 / 1000: loss 371.699141\n",
      "iteration 100 / 1000: loss 3.924230\n",
      "iteration 200 / 1000: loss 3.774982\n",
      "iteration 300 / 1000: loss 7.866347\n",
      "iteration 400 / 1000: loss 6.184708\n",
      "iteration 500 / 1000: loss 6.790934\n",
      "iteration 600 / 1000: loss 6.600311\n",
      "iteration 700 / 1000: loss 5.378882\n",
      "iteration 800 / 1000: loss 4.097317\n",
      "iteration 900 / 1000: loss 4.600282\n",
      "training accuracy: 0.1896734694\n",
      "validation accuracy: 0.1820000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000100000\n",
      "regularization_strengths: 25000.0000000000\n",
      "iteration 0 / 1000: loss 390.738575\n",
      "iteration 100 / 1000: loss 4.871928\n",
      "iteration 200 / 1000: loss 5.759978\n",
      "iteration 300 / 1000: loss 5.664737\n",
      "iteration 400 / 1000: loss 8.012209\n",
      "iteration 500 / 1000: loss 5.981952\n",
      "iteration 600 / 1000: loss 6.295050\n",
      "iteration 700 / 1000: loss 5.239639\n",
      "iteration 800 / 1000: loss 4.050420\n",
      "iteration 900 / 1000: loss 5.758076\n",
      "training accuracy: 0.1413061224\n",
      "validation accuracy: 0.1480000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000100000\n",
      "regularization_strengths: 30000.0000000000\n",
      "iteration 0 / 1000: loss 475.217384\n",
      "iteration 100 / 1000: loss 7.358154\n",
      "iteration 200 / 1000: loss 4.894653\n",
      "iteration 300 / 1000: loss 5.944277\n",
      "iteration 400 / 1000: loss 7.117668\n",
      "iteration 500 / 1000: loss 5.068876\n",
      "iteration 600 / 1000: loss 5.967469\n",
      "iteration 700 / 1000: loss 4.816724\n",
      "iteration 800 / 1000: loss 4.879758\n",
      "iteration 900 / 1000: loss 6.784278\n",
      "training accuracy: 0.1274081633\n",
      "validation accuracy: 0.1200000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000100000\n",
      "regularization_strengths: 40000.0000000000\n",
      "iteration 0 / 1000: loss 617.048124\n",
      "iteration 100 / 1000: loss 7.320840\n",
      "iteration 200 / 1000: loss 6.555539\n",
      "iteration 300 / 1000: loss 5.692137\n",
      "iteration 400 / 1000: loss 6.930848\n",
      "iteration 500 / 1000: loss 7.328473\n",
      "iteration 600 / 1000: loss 5.294905\n",
      "iteration 700 / 1000: loss 9.785971\n",
      "iteration 800 / 1000: loss 6.443129\n",
      "iteration 900 / 1000: loss 5.607038\n",
      "training accuracy: 0.1428979592\n",
      "validation accuracy: 0.1460000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000500000\n",
      "regularization_strengths: 5000.0000000000\n",
      "iteration 0 / 1000: loss 82.479811\n",
      "iteration 100 / 1000: loss 32.731103\n",
      "iteration 200 / 1000: loss 25.188072\n",
      "iteration 300 / 1000: loss 53.293726\n",
      "iteration 400 / 1000: loss 20.229905\n",
      "iteration 500 / 1000: loss 24.950254\n",
      "iteration 600 / 1000: loss 21.155577\n",
      "iteration 700 / 1000: loss 38.780640\n",
      "iteration 800 / 1000: loss 27.261947\n",
      "iteration 900 / 1000: loss 23.857666\n",
      "training accuracy: 0.1841020408\n",
      "validation accuracy: 0.1840000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000500000\n",
      "regularization_strengths: 10000.0000000000\n",
      "iteration 0 / 1000: loss 160.442268\n",
      "iteration 100 / 1000: loss 43.969330\n",
      "iteration 200 / 1000: loss 49.345220\n",
      "iteration 300 / 1000: loss 42.269483\n",
      "iteration 400 / 1000: loss 46.202044\n",
      "iteration 500 / 1000: loss 33.435368\n",
      "iteration 600 / 1000: loss 46.151892\n",
      "iteration 700 / 1000: loss 29.683200\n",
      "iteration 800 / 1000: loss 38.545509\n",
      "iteration 900 / 1000: loss 40.416264\n",
      "training accuracy: 0.1678163265\n",
      "validation accuracy: 0.1550000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000500000\n",
      "regularization_strengths: 11000.0000000000\n",
      "iteration 0 / 1000: loss 174.728783\n",
      "iteration 100 / 1000: loss 35.326612\n",
      "iteration 200 / 1000: loss 50.783313\n",
      "iteration 300 / 1000: loss 35.701814\n",
      "iteration 400 / 1000: loss 37.943244\n",
      "iteration 500 / 1000: loss 18.570588\n",
      "iteration 600 / 1000: loss 49.954467\n",
      "iteration 700 / 1000: loss 36.935111\n",
      "iteration 800 / 1000: loss 55.943129\n",
      "iteration 900 / 1000: loss 37.301459\n",
      "training accuracy: 0.0930204082\n",
      "validation accuracy: 0.0750000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000500000\n",
      "regularization_strengths: 12000.0000000000\n",
      "iteration 0 / 1000: loss 193.243861\n",
      "iteration 100 / 1000: loss 49.732785\n",
      "iteration 200 / 1000: loss 39.206897\n",
      "iteration 300 / 1000: loss 47.329410\n",
      "iteration 400 / 1000: loss 37.297883\n",
      "iteration 500 / 1000: loss 43.852034\n",
      "iteration 600 / 1000: loss 38.993525\n",
      "iteration 700 / 1000: loss 32.943615\n",
      "iteration 800 / 1000: loss 66.016366\n",
      "iteration 900 / 1000: loss 44.437560\n",
      "training accuracy: 0.1309591837\n",
      "validation accuracy: 0.1220000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000500000\n",
      "regularization_strengths: 13000.0000000000\n",
      "iteration 0 / 1000: loss 206.132270\n",
      "iteration 100 / 1000: loss 44.866811\n",
      "iteration 200 / 1000: loss 52.796510\n",
      "iteration 300 / 1000: loss 49.486154\n",
      "iteration 400 / 1000: loss 55.894134\n",
      "iteration 500 / 1000: loss 45.743818\n",
      "iteration 600 / 1000: loss 39.573083\n",
      "iteration 700 / 1000: loss 65.887419\n",
      "iteration 800 / 1000: loss 60.930821\n",
      "iteration 900 / 1000: loss 55.324419\n",
      "training accuracy: 0.1446326531\n",
      "validation accuracy: 0.1680000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000500000\n",
      "regularization_strengths: 14000.0000000000\n",
      "iteration 0 / 1000: loss 221.694899\n",
      "iteration 100 / 1000: loss 53.025335\n",
      "iteration 200 / 1000: loss 59.405465\n",
      "iteration 300 / 1000: loss 53.839520\n",
      "iteration 400 / 1000: loss 46.835371\n",
      "iteration 500 / 1000: loss 42.850354\n",
      "iteration 600 / 1000: loss 57.758023\n",
      "iteration 700 / 1000: loss 66.481559\n",
      "iteration 800 / 1000: loss 47.620069\n",
      "iteration 900 / 1000: loss 48.514144\n",
      "training accuracy: 0.0842448980\n",
      "validation accuracy: 0.0790000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000500000\n",
      "regularization_strengths: 15000.0000000000\n",
      "iteration 0 / 1000: loss 238.223612\n",
      "iteration 100 / 1000: loss 55.518630\n",
      "iteration 200 / 1000: loss 56.361818\n",
      "iteration 300 / 1000: loss 57.314278\n",
      "iteration 400 / 1000: loss 45.802476\n",
      "iteration 500 / 1000: loss 49.132151\n",
      "iteration 600 / 1000: loss 63.900292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 700 / 1000: loss 56.864612\n",
      "iteration 800 / 1000: loss 59.815258\n",
      "iteration 900 / 1000: loss 59.281944\n",
      "training accuracy: 0.1067346939\n",
      "validation accuracy: 0.0940000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000500000\n",
      "regularization_strengths: 16000.0000000000\n",
      "iteration 0 / 1000: loss 250.613451\n",
      "iteration 100 / 1000: loss 54.143263\n",
      "iteration 200 / 1000: loss 66.171374\n",
      "iteration 300 / 1000: loss 65.906391\n",
      "iteration 400 / 1000: loss 51.004969\n",
      "iteration 500 / 1000: loss 71.209090\n",
      "iteration 600 / 1000: loss 67.282104\n",
      "iteration 700 / 1000: loss 61.918034\n",
      "iteration 800 / 1000: loss 68.691753\n",
      "iteration 900 / 1000: loss 71.183473\n",
      "training accuracy: 0.1071632653\n",
      "validation accuracy: 0.0960000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000500000\n",
      "regularization_strengths: 17000.0000000000\n",
      "iteration 0 / 1000: loss 269.113310\n",
      "iteration 100 / 1000: loss 66.614471\n",
      "iteration 200 / 1000: loss 71.405825\n",
      "iteration 300 / 1000: loss 76.969744\n",
      "iteration 400 / 1000: loss 61.080281\n",
      "iteration 500 / 1000: loss 79.696144\n",
      "iteration 600 / 1000: loss 75.733734\n",
      "iteration 700 / 1000: loss 72.115038\n",
      "iteration 800 / 1000: loss 72.949025\n",
      "iteration 900 / 1000: loss 76.650610\n",
      "training accuracy: 0.1375918367\n",
      "validation accuracy: 0.1440000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000500000\n",
      "regularization_strengths: 18000.0000000000\n",
      "iteration 0 / 1000: loss 281.968107\n",
      "iteration 100 / 1000: loss 90.317870\n",
      "iteration 200 / 1000: loss 68.148535\n",
      "iteration 300 / 1000: loss 72.833091\n",
      "iteration 400 / 1000: loss 73.411162\n",
      "iteration 500 / 1000: loss 74.796237\n",
      "iteration 600 / 1000: loss 75.997413\n",
      "iteration 700 / 1000: loss 73.804087\n",
      "iteration 800 / 1000: loss 70.151785\n",
      "iteration 900 / 1000: loss 76.010791\n",
      "training accuracy: 0.1336734694\n",
      "validation accuracy: 0.1420000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000500000\n",
      "regularization_strengths: 19000.0000000000\n",
      "iteration 0 / 1000: loss 294.508027\n",
      "iteration 100 / 1000: loss 89.812868\n",
      "iteration 200 / 1000: loss 85.452183\n",
      "iteration 300 / 1000: loss 83.862459\n",
      "iteration 400 / 1000: loss 86.049512\n",
      "iteration 500 / 1000: loss 85.047832\n",
      "iteration 600 / 1000: loss 96.771588\n",
      "iteration 700 / 1000: loss 81.025202\n",
      "iteration 800 / 1000: loss 70.344577\n",
      "iteration 900 / 1000: loss 87.251167\n",
      "training accuracy: 0.1208163265\n",
      "validation accuracy: 0.1160000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000500000\n",
      "regularization_strengths: 20000.0000000000\n",
      "iteration 0 / 1000: loss 311.162789\n",
      "iteration 100 / 1000: loss 76.391126\n",
      "iteration 200 / 1000: loss 76.480610\n",
      "iteration 300 / 1000: loss 67.809777\n",
      "iteration 400 / 1000: loss 81.702161\n",
      "iteration 500 / 1000: loss 81.702861\n",
      "iteration 600 / 1000: loss 107.274326\n",
      "iteration 700 / 1000: loss 71.670172\n",
      "iteration 800 / 1000: loss 85.500894\n",
      "iteration 900 / 1000: loss 73.492179\n",
      "training accuracy: 0.0651428571\n",
      "validation accuracy: 0.0620000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000500000\n",
      "regularization_strengths: 21000.0000000000\n",
      "iteration 0 / 1000: loss 324.620280\n",
      "iteration 100 / 1000: loss 103.060731\n",
      "iteration 200 / 1000: loss 79.994988\n",
      "iteration 300 / 1000: loss 116.587966\n",
      "iteration 400 / 1000: loss 97.453652\n",
      "iteration 500 / 1000: loss 92.490804\n",
      "iteration 600 / 1000: loss 82.681261\n",
      "iteration 700 / 1000: loss 85.222256\n",
      "iteration 800 / 1000: loss 92.151084\n",
      "iteration 900 / 1000: loss 84.774414\n",
      "training accuracy: 0.0811020408\n",
      "validation accuracy: 0.0860000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000500000\n",
      "regularization_strengths: 22000.0000000000\n",
      "iteration 0 / 1000: loss 344.923413\n",
      "iteration 100 / 1000: loss 91.760537\n",
      "iteration 200 / 1000: loss 91.065913\n",
      "iteration 300 / 1000: loss 92.117094\n",
      "iteration 400 / 1000: loss 95.471117\n",
      "iteration 500 / 1000: loss 99.400533\n",
      "iteration 600 / 1000: loss 89.174652\n",
      "iteration 700 / 1000: loss 92.408391\n",
      "iteration 800 / 1000: loss 108.084335\n",
      "iteration 900 / 1000: loss 108.334199\n",
      "training accuracy: 0.0803265306\n",
      "validation accuracy: 0.0550000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000500000\n",
      "regularization_strengths: 23000.0000000000\n",
      "iteration 0 / 1000: loss 359.274887\n",
      "iteration 100 / 1000: loss 102.805781\n",
      "iteration 200 / 1000: loss 109.923707\n",
      "iteration 300 / 1000: loss 117.018184\n",
      "iteration 400 / 1000: loss 121.663023\n",
      "iteration 500 / 1000: loss 108.107517\n",
      "iteration 600 / 1000: loss 115.149992\n",
      "iteration 700 / 1000: loss 126.000777\n",
      "iteration 800 / 1000: loss 113.983884\n",
      "iteration 900 / 1000: loss 122.395285\n",
      "training accuracy: 0.0957755102\n",
      "validation accuracy: 0.0930000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000500000\n",
      "regularization_strengths: 24000.0000000000\n",
      "iteration 0 / 1000: loss 371.473894\n",
      "iteration 100 / 1000: loss 124.311314\n",
      "iteration 200 / 1000: loss 129.610219\n",
      "iteration 300 / 1000: loss 122.149234\n",
      "iteration 400 / 1000: loss 112.420052\n",
      "iteration 500 / 1000: loss 110.651369\n",
      "iteration 600 / 1000: loss 125.628538\n",
      "iteration 700 / 1000: loss 130.006538\n",
      "iteration 800 / 1000: loss 145.641289\n",
      "iteration 900 / 1000: loss 135.018187\n",
      "training accuracy: 0.0808571429\n",
      "validation accuracy: 0.0850000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000500000\n",
      "regularization_strengths: 25000.0000000000\n",
      "iteration 0 / 1000: loss 389.583078\n",
      "iteration 100 / 1000: loss 127.069587\n",
      "iteration 200 / 1000: loss 159.147897\n",
      "iteration 300 / 1000: loss 124.720098\n",
      "iteration 400 / 1000: loss 133.489409\n",
      "iteration 500 / 1000: loss 153.982427\n",
      "iteration 600 / 1000: loss 137.198589\n",
      "iteration 700 / 1000: loss 154.593234\n",
      "iteration 800 / 1000: loss 134.153681\n",
      "iteration 900 / 1000: loss 142.870361\n",
      "training accuracy: 0.0642040816\n",
      "validation accuracy: 0.0640000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000500000\n",
      "regularization_strengths: 30000.0000000000\n",
      "iteration 0 / 1000: loss 465.113878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yxue/stanford-cs231n-spring1718/assignment1/cs231n/classifiers/softmax.py:78: RuntimeWarning: divide by zero encountered in log\n",
      "  cross_entropy_loss = -np.log(softmax[np.arange(softmax.shape[0]), y])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 1000: loss 285.464823\n",
      "iteration 200 / 1000: loss 287.037028\n",
      "iteration 300 / 1000: loss 330.751269\n",
      "iteration 400 / 1000: loss 300.786665\n",
      "iteration 500 / 1000: loss 311.840666\n",
      "iteration 600 / 1000: loss 264.018426\n",
      "iteration 700 / 1000: loss inf\n",
      "iteration 800 / 1000: loss 276.452952\n",
      "iteration 900 / 1000: loss 285.238838\n",
      "training accuracy: 0.0653877551\n",
      "validation accuracy: 0.0680000000\n",
      "------------------------------\n",
      "learning_rate: 0.0000500000\n",
      "regularization_strengths: 40000.0000000000\n",
      "iteration 0 / 1000: loss 620.393752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yxue/stanford-cs231n-spring1718/assignment1/cs231n/classifiers/softmax.py:77: RuntimeWarning: overflow encountered in exp\n",
      "  softmax = np.exp(X.dot(W)) / np.sum(np.exp(X.dot(W)), axis=1, keepdims=True)\n",
      "/home/yxue/stanford-cs231n-spring1718/assignment1/cs231n/classifiers/softmax.py:77: RuntimeWarning: invalid value encountered in true_divide\n",
      "  softmax = np.exp(X.dot(W)) / np.sum(np.exp(X.dot(W)), axis=1, keepdims=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.1002653061\n",
      "validation accuracy: 0.0870000000\n",
      "------------------------------\n",
      "lr 1.000000e-07 reg 5.000000e+03 train accuracy: 0.260469 val accuracy: 0.249000\n",
      "lr 1.000000e-07 reg 1.000000e+04 train accuracy: 0.286653 val accuracy: 0.287000\n",
      "lr 1.000000e-07 reg 1.100000e+04 train accuracy: 0.297122 val accuracy: 0.299000\n",
      "lr 1.000000e-07 reg 1.200000e+04 train accuracy: 0.297449 val accuracy: 0.291000\n",
      "lr 1.000000e-07 reg 1.300000e+04 train accuracy: 0.306633 val accuracy: 0.315000\n",
      "lr 1.000000e-07 reg 1.400000e+04 train accuracy: 0.307102 val accuracy: 0.303000\n",
      "lr 1.000000e-07 reg 1.500000e+04 train accuracy: 0.314245 val accuracy: 0.326000\n",
      "lr 1.000000e-07 reg 1.600000e+04 train accuracy: 0.321347 val accuracy: 0.336000\n",
      "lr 1.000000e-07 reg 1.700000e+04 train accuracy: 0.320061 val accuracy: 0.326000\n",
      "lr 1.000000e-07 reg 1.800000e+04 train accuracy: 0.322816 val accuracy: 0.305000\n",
      "lr 1.000000e-07 reg 1.900000e+04 train accuracy: 0.330204 val accuracy: 0.349000\n",
      "lr 1.000000e-07 reg 2.000000e+04 train accuracy: 0.324776 val accuracy: 0.341000\n",
      "lr 1.000000e-07 reg 2.100000e+04 train accuracy: 0.331551 val accuracy: 0.330000\n",
      "lr 1.000000e-07 reg 2.200000e+04 train accuracy: 0.334653 val accuracy: 0.355000\n",
      "lr 1.000000e-07 reg 2.300000e+04 train accuracy: 0.334714 val accuracy: 0.341000\n",
      "lr 1.000000e-07 reg 2.400000e+04 train accuracy: 0.334531 val accuracy: 0.350000\n",
      "lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.334163 val accuracy: 0.355000\n",
      "lr 1.000000e-07 reg 3.000000e+04 train accuracy: 0.339755 val accuracy: 0.358000\n",
      "lr 1.000000e-07 reg 4.000000e+04 train accuracy: 0.334102 val accuracy: 0.350000\n",
      "lr 4.000000e-07 reg 5.000000e+03 train accuracy: 0.377592 val accuracy: 0.366000\n",
      "lr 4.000000e-07 reg 1.000000e+04 train accuracy: 0.370571 val accuracy: 0.374000\n",
      "lr 4.000000e-07 reg 1.100000e+04 train accuracy: 0.367265 val accuracy: 0.391000\n",
      "lr 4.000000e-07 reg 1.200000e+04 train accuracy: 0.372714 val accuracy: 0.381000\n",
      "lr 4.000000e-07 reg 1.300000e+04 train accuracy: 0.365776 val accuracy: 0.377000\n",
      "lr 4.000000e-07 reg 1.400000e+04 train accuracy: 0.365837 val accuracy: 0.377000\n",
      "lr 4.000000e-07 reg 1.500000e+04 train accuracy: 0.359776 val accuracy: 0.372000\n",
      "lr 4.000000e-07 reg 1.600000e+04 train accuracy: 0.358592 val accuracy: 0.386000\n",
      "lr 4.000000e-07 reg 1.700000e+04 train accuracy: 0.358531 val accuracy: 0.372000\n",
      "lr 4.000000e-07 reg 1.800000e+04 train accuracy: 0.358673 val accuracy: 0.372000\n",
      "lr 4.000000e-07 reg 1.900000e+04 train accuracy: 0.358163 val accuracy: 0.365000\n",
      "lr 4.000000e-07 reg 2.000000e+04 train accuracy: 0.361449 val accuracy: 0.371000\n",
      "lr 4.000000e-07 reg 2.100000e+04 train accuracy: 0.358286 val accuracy: 0.372000\n",
      "lr 4.000000e-07 reg 2.200000e+04 train accuracy: 0.340041 val accuracy: 0.351000\n",
      "lr 4.000000e-07 reg 2.300000e+04 train accuracy: 0.345796 val accuracy: 0.363000\n",
      "lr 4.000000e-07 reg 2.400000e+04 train accuracy: 0.354490 val accuracy: 0.369000\n",
      "lr 4.000000e-07 reg 2.500000e+04 train accuracy: 0.352898 val accuracy: 0.373000\n",
      "lr 4.000000e-07 reg 3.000000e+04 train accuracy: 0.343653 val accuracy: 0.353000\n",
      "lr 4.000000e-07 reg 4.000000e+04 train accuracy: 0.333714 val accuracy: 0.337000\n",
      "lr 8.000000e-07 reg 5.000000e+03 train accuracy: 0.381612 val accuracy: 0.385000\n",
      "lr 8.000000e-07 reg 1.000000e+04 train accuracy: 0.370490 val accuracy: 0.372000\n",
      "lr 8.000000e-07 reg 1.100000e+04 train accuracy: 0.359204 val accuracy: 0.377000\n",
      "lr 8.000000e-07 reg 1.200000e+04 train accuracy: 0.366755 val accuracy: 0.375000\n",
      "lr 8.000000e-07 reg 1.300000e+04 train accuracy: 0.357959 val accuracy: 0.375000\n",
      "lr 8.000000e-07 reg 1.400000e+04 train accuracy: 0.359224 val accuracy: 0.372000\n",
      "lr 8.000000e-07 reg 1.500000e+04 train accuracy: 0.366694 val accuracy: 0.381000\n",
      "lr 8.000000e-07 reg 1.600000e+04 train accuracy: 0.349776 val accuracy: 0.371000\n",
      "lr 8.000000e-07 reg 1.700000e+04 train accuracy: 0.356490 val accuracy: 0.369000\n",
      "lr 8.000000e-07 reg 1.800000e+04 train accuracy: 0.354796 val accuracy: 0.377000\n",
      "lr 8.000000e-07 reg 1.900000e+04 train accuracy: 0.351694 val accuracy: 0.372000\n",
      "lr 8.000000e-07 reg 2.000000e+04 train accuracy: 0.348592 val accuracy: 0.359000\n",
      "lr 8.000000e-07 reg 2.100000e+04 train accuracy: 0.352531 val accuracy: 0.368000\n",
      "lr 8.000000e-07 reg 2.200000e+04 train accuracy: 0.344000 val accuracy: 0.348000\n",
      "lr 8.000000e-07 reg 2.300000e+04 train accuracy: 0.354163 val accuracy: 0.363000\n",
      "lr 8.000000e-07 reg 2.400000e+04 train accuracy: 0.339265 val accuracy: 0.348000\n",
      "lr 8.000000e-07 reg 2.500000e+04 train accuracy: 0.331980 val accuracy: 0.352000\n",
      "lr 8.000000e-07 reg 3.000000e+04 train accuracy: 0.342204 val accuracy: 0.364000\n",
      "lr 8.000000e-07 reg 4.000000e+04 train accuracy: 0.326735 val accuracy: 0.346000\n",
      "lr 1.000000e-06 reg 5.000000e+03 train accuracy: 0.385959 val accuracy: 0.387000\n",
      "lr 1.000000e-06 reg 1.000000e+04 train accuracy: 0.363082 val accuracy: 0.376000\n",
      "lr 1.000000e-06 reg 1.100000e+04 train accuracy: 0.368408 val accuracy: 0.378000\n",
      "lr 1.000000e-06 reg 1.200000e+04 train accuracy: 0.364592 val accuracy: 0.381000\n",
      "lr 1.000000e-06 reg 1.300000e+04 train accuracy: 0.363061 val accuracy: 0.366000\n",
      "lr 1.000000e-06 reg 1.400000e+04 train accuracy: 0.362143 val accuracy: 0.367000\n",
      "lr 1.000000e-06 reg 1.500000e+04 train accuracy: 0.355122 val accuracy: 0.366000\n",
      "lr 1.000000e-06 reg 1.600000e+04 train accuracy: 0.360571 val accuracy: 0.365000\n",
      "lr 1.000000e-06 reg 1.700000e+04 train accuracy: 0.355000 val accuracy: 0.370000\n",
      "lr 1.000000e-06 reg 1.800000e+04 train accuracy: 0.349143 val accuracy: 0.341000\n",
      "lr 1.000000e-06 reg 1.900000e+04 train accuracy: 0.347735 val accuracy: 0.367000\n",
      "lr 1.000000e-06 reg 2.000000e+04 train accuracy: 0.349857 val accuracy: 0.355000\n",
      "lr 1.000000e-06 reg 2.100000e+04 train accuracy: 0.347653 val accuracy: 0.357000\n",
      "lr 1.000000e-06 reg 2.200000e+04 train accuracy: 0.350265 val accuracy: 0.352000\n",
      "lr 1.000000e-06 reg 2.300000e+04 train accuracy: 0.355388 val accuracy: 0.369000\n",
      "lr 1.000000e-06 reg 2.400000e+04 train accuracy: 0.342837 val accuracy: 0.342000\n",
      "lr 1.000000e-06 reg 2.500000e+04 train accuracy: 0.344816 val accuracy: 0.343000\n",
      "lr 1.000000e-06 reg 3.000000e+04 train accuracy: 0.335755 val accuracy: 0.344000\n",
      "lr 1.000000e-06 reg 4.000000e+04 train accuracy: 0.337367 val accuracy: 0.348000\n",
      "lr 4.000000e-06 reg 5.000000e+03 train accuracy: 0.334694 val accuracy: 0.332000\n",
      "lr 4.000000e-06 reg 1.000000e+04 train accuracy: 0.328816 val accuracy: 0.331000\n",
      "lr 4.000000e-06 reg 1.100000e+04 train accuracy: 0.316612 val accuracy: 0.332000\n",
      "lr 4.000000e-06 reg 1.200000e+04 train accuracy: 0.330755 val accuracy: 0.333000\n",
      "lr 4.000000e-06 reg 1.300000e+04 train accuracy: 0.314551 val accuracy: 0.321000\n",
      "lr 4.000000e-06 reg 1.400000e+04 train accuracy: 0.333755 val accuracy: 0.328000\n",
      "lr 4.000000e-06 reg 1.500000e+04 train accuracy: 0.278306 val accuracy: 0.284000\n",
      "lr 4.000000e-06 reg 1.600000e+04 train accuracy: 0.320980 val accuracy: 0.306000\n",
      "lr 4.000000e-06 reg 1.700000e+04 train accuracy: 0.297122 val accuracy: 0.323000\n",
      "lr 4.000000e-06 reg 1.800000e+04 train accuracy: 0.320143 val accuracy: 0.321000\n",
      "lr 4.000000e-06 reg 1.900000e+04 train accuracy: 0.291245 val accuracy: 0.316000\n",
      "lr 4.000000e-06 reg 2.000000e+04 train accuracy: 0.281612 val accuracy: 0.293000\n",
      "lr 4.000000e-06 reg 2.100000e+04 train accuracy: 0.262082 val accuracy: 0.268000\n",
      "lr 4.000000e-06 reg 2.200000e+04 train accuracy: 0.300000 val accuracy: 0.310000\n",
      "lr 4.000000e-06 reg 2.300000e+04 train accuracy: 0.289918 val accuracy: 0.302000\n",
      "lr 4.000000e-06 reg 2.400000e+04 train accuracy: 0.280286 val accuracy: 0.304000\n",
      "lr 4.000000e-06 reg 2.500000e+04 train accuracy: 0.286714 val accuracy: 0.298000\n",
      "lr 4.000000e-06 reg 3.000000e+04 train accuracy: 0.275143 val accuracy: 0.280000\n",
      "lr 4.000000e-06 reg 4.000000e+04 train accuracy: 0.295061 val accuracy: 0.290000\n",
      "lr 8.000000e-06 reg 5.000000e+03 train accuracy: 0.223918 val accuracy: 0.212000\n",
      "lr 8.000000e-06 reg 1.000000e+04 train accuracy: 0.198082 val accuracy: 0.203000\n",
      "lr 8.000000e-06 reg 1.100000e+04 train accuracy: 0.216592 val accuracy: 0.227000\n",
      "lr 8.000000e-06 reg 1.200000e+04 train accuracy: 0.229898 val accuracy: 0.242000\n",
      "lr 8.000000e-06 reg 1.300000e+04 train accuracy: 0.250939 val accuracy: 0.260000\n",
      "lr 8.000000e-06 reg 1.400000e+04 train accuracy: 0.192122 val accuracy: 0.185000\n",
      "lr 8.000000e-06 reg 1.500000e+04 train accuracy: 0.169857 val accuracy: 0.178000\n",
      "lr 8.000000e-06 reg 1.600000e+04 train accuracy: 0.240878 val accuracy: 0.238000\n",
      "lr 8.000000e-06 reg 1.700000e+04 train accuracy: 0.234633 val accuracy: 0.233000\n",
      "lr 8.000000e-06 reg 1.800000e+04 train accuracy: 0.241939 val accuracy: 0.253000\n",
      "lr 8.000000e-06 reg 1.900000e+04 train accuracy: 0.250204 val accuracy: 0.244000\n",
      "lr 8.000000e-06 reg 2.000000e+04 train accuracy: 0.224878 val accuracy: 0.249000\n",
      "lr 8.000000e-06 reg 2.100000e+04 train accuracy: 0.164020 val accuracy: 0.169000\n",
      "lr 8.000000e-06 reg 2.200000e+04 train accuracy: 0.176000 val accuracy: 0.194000\n",
      "lr 8.000000e-06 reg 2.300000e+04 train accuracy: 0.202143 val accuracy: 0.217000\n",
      "lr 8.000000e-06 reg 2.400000e+04 train accuracy: 0.151184 val accuracy: 0.165000\n",
      "lr 8.000000e-06 reg 2.500000e+04 train accuracy: 0.195265 val accuracy: 0.171000\n",
      "lr 8.000000e-06 reg 3.000000e+04 train accuracy: 0.186796 val accuracy: 0.194000\n",
      "lr 8.000000e-06 reg 4.000000e+04 train accuracy: 0.138755 val accuracy: 0.136000\n",
      "lr 1.000000e-05 reg 5.000000e+03 train accuracy: 0.255265 val accuracy: 0.267000\n",
      "lr 1.000000e-05 reg 1.000000e+04 train accuracy: 0.231551 val accuracy: 0.239000\n",
      "lr 1.000000e-05 reg 1.100000e+04 train accuracy: 0.189694 val accuracy: 0.202000\n",
      "lr 1.000000e-05 reg 1.200000e+04 train accuracy: 0.212857 val accuracy: 0.226000\n",
      "lr 1.000000e-05 reg 1.300000e+04 train accuracy: 0.203796 val accuracy: 0.204000\n",
      "lr 1.000000e-05 reg 1.400000e+04 train accuracy: 0.186204 val accuracy: 0.189000\n",
      "lr 1.000000e-05 reg 1.500000e+04 train accuracy: 0.186408 val accuracy: 0.206000\n",
      "lr 1.000000e-05 reg 1.600000e+04 train accuracy: 0.178061 val accuracy: 0.196000\n",
      "lr 1.000000e-05 reg 1.700000e+04 train accuracy: 0.162327 val accuracy: 0.164000\n",
      "lr 1.000000e-05 reg 1.800000e+04 train accuracy: 0.252776 val accuracy: 0.247000\n",
      "lr 1.000000e-05 reg 1.900000e+04 train accuracy: 0.165735 val accuracy: 0.181000\n",
      "lr 1.000000e-05 reg 2.000000e+04 train accuracy: 0.200816 val accuracy: 0.202000\n",
      "lr 1.000000e-05 reg 2.100000e+04 train accuracy: 0.119837 val accuracy: 0.104000\n",
      "lr 1.000000e-05 reg 2.200000e+04 train accuracy: 0.189184 val accuracy: 0.195000\n",
      "lr 1.000000e-05 reg 2.300000e+04 train accuracy: 0.155347 val accuracy: 0.182000\n",
      "lr 1.000000e-05 reg 2.400000e+04 train accuracy: 0.189673 val accuracy: 0.182000\n",
      "lr 1.000000e-05 reg 2.500000e+04 train accuracy: 0.141306 val accuracy: 0.148000\n",
      "lr 1.000000e-05 reg 3.000000e+04 train accuracy: 0.127408 val accuracy: 0.120000\n",
      "lr 1.000000e-05 reg 4.000000e+04 train accuracy: 0.142898 val accuracy: 0.146000\n",
      "lr 5.000000e-05 reg 5.000000e+03 train accuracy: 0.184102 val accuracy: 0.184000\n",
      "lr 5.000000e-05 reg 1.000000e+04 train accuracy: 0.167816 val accuracy: 0.155000\n",
      "lr 5.000000e-05 reg 1.100000e+04 train accuracy: 0.093020 val accuracy: 0.075000\n",
      "lr 5.000000e-05 reg 1.200000e+04 train accuracy: 0.130959 val accuracy: 0.122000\n",
      "lr 5.000000e-05 reg 1.300000e+04 train accuracy: 0.144633 val accuracy: 0.168000\n",
      "lr 5.000000e-05 reg 1.400000e+04 train accuracy: 0.084245 val accuracy: 0.079000\n",
      "lr 5.000000e-05 reg 1.500000e+04 train accuracy: 0.106735 val accuracy: 0.094000\n",
      "lr 5.000000e-05 reg 1.600000e+04 train accuracy: 0.107163 val accuracy: 0.096000\n",
      "lr 5.000000e-05 reg 1.700000e+04 train accuracy: 0.137592 val accuracy: 0.144000\n",
      "lr 5.000000e-05 reg 1.800000e+04 train accuracy: 0.133673 val accuracy: 0.142000\n",
      "lr 5.000000e-05 reg 1.900000e+04 train accuracy: 0.120816 val accuracy: 0.116000\n",
      "lr 5.000000e-05 reg 2.000000e+04 train accuracy: 0.065143 val accuracy: 0.062000\n",
      "lr 5.000000e-05 reg 2.100000e+04 train accuracy: 0.081102 val accuracy: 0.086000\n",
      "lr 5.000000e-05 reg 2.200000e+04 train accuracy: 0.080327 val accuracy: 0.055000\n",
      "lr 5.000000e-05 reg 2.300000e+04 train accuracy: 0.095776 val accuracy: 0.093000\n",
      "lr 5.000000e-05 reg 2.400000e+04 train accuracy: 0.080857 val accuracy: 0.085000\n",
      "lr 5.000000e-05 reg 2.500000e+04 train accuracy: 0.064204 val accuracy: 0.064000\n",
      "lr 5.000000e-05 reg 3.000000e+04 train accuracy: 0.065388 val accuracy: 0.068000\n",
      "lr 5.000000e-05 reg 4.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "best validation accuracy achieved during cross-validation: 0.391000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 4e-7, 8e-7, 1e-6, 4e-6, 8e-6, 1e-5, 5e-5]\n",
    "regularization_strengths = [0.5e4, 1e4, \n",
    "                            1.1e4, 1.2e4, 1.3e4, 1.4e4, 1.5e4, 1.6e4, 1.7e4, 1.8e4, 1.9e4, \n",
    "                            2e4, 2.1e4, 2.2e4, 2.3e4, 2.4e4, 2.5e4, \n",
    "                            3e4, 4e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "for lr in learning_rates:\n",
    "    for reg in regularization_strengths:\n",
    "        print('learning_rate: %.10f' % lr)\n",
    "        print('regularization_strengths: %.10f' % reg)\n",
    "        softmax = Softmax() # Always create a new Softmax Object\n",
    "        softmax.train(X_train, y_train, learning_rate=lr, reg=reg, num_iters=1000, verbose=True)\n",
    "        \n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        training_accuracy = np.mean(y_train == y_train_pred)\n",
    "        print('training accuracy: %.10f' % (np.mean(y_train == y_train_pred), ))\n",
    "                             \n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        validation_accuracy = np.mean(y_val == y_val_pred)               \n",
    "        print('validation accuracy: %.10f' % (np.mean(y_val == y_val_pred), ))\n",
    "        \n",
    "        results[(lr, reg)] = (training_accuracy, validation_accuracy)\n",
    "        if validation_accuracy > best_val:\n",
    "            best_val = validation_accuracy\n",
    "            best_softmax = softmax\n",
    "        print(\"------------------------------\")\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.373000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inline Question** - *True or False*\n",
    "\n",
    "It's possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "*Your answer*:\n",
    "- True\n",
    "\n",
    "*Your explanation*:\n",
    "- if the new datapoint's score is out of the margin range from the correct class score the SVM loss wouldn't change, but in the Softmax loss if the score of the new added datapoint be close to positive infinity it will make the softmax value of correct class very small which makes the loss extremly huge, so definitely affect the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f119e3e5d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
